{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe38b14",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# problem_6.py\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def _build_q_to_kv_map(Hq, Hkv):\n",
    "    # Mirror the original integer-division mapping used in the naive reference\n",
    "    if Hq % Hkv != 0:\n",
    "        group = max(1, Hq // Hkv)\n",
    "    else:\n",
    "        group = Hq // Hkv\n",
    "    mapping = [min(hq // group, Hkv - 1) for hq in range(Hq)]\n",
    "    return torch.tensor(mapping, dtype=torch.long)\n",
    "\n",
    "def flash_attention_forward(q, k, v, is_causal=True, window_size=128):\n",
    "    \"\"\"\n",
    "    Vectorized implementation for Sliding Window Attention (Problem 6)\n",
    "    - Preserves the original GQA mapping semantics used by the naive reference\n",
    "    - Supports causal or non-causal with sliding window of size `window_size`\n",
    "    - q: (B, Hq, L, D)\n",
    "    - k: (B, Hkv, L, D)\n",
    "    - v: (B, Hkv, L, D)\n",
    "    Returns:\n",
    "    - out: (B, Hq, L, D)\n",
    "    \"\"\"\n",
    "    assert q.dim() == 4 and k.dim() == 4 and v.dim() == 4, \"q/k/v must be 4D tensors\"\n",
    "    B, Hq, Lq, D = q.shape\n",
    "    _, Hkv, Lk, Dk = k.shape\n",
    "    assert D == Dk, \"query/key head dim mismatch\"\n",
    "    assert Lq == Lk, \"sequence lengths of q and k/v must match for sliding attention\"\n",
    "\n",
    "    device = q.device\n",
    "    dtype = q.dtype\n",
    "\n",
    "    # Build q->kv mapping exactly like the naive implementation\n",
    "    mapping = _build_q_to_kv_map(Hq, Hkv).to(device)  # length Hq, values in [0, Hkv-1]\n",
    "\n",
    "    # Expand K and V to have Hq heads by indexing with mapping\n",
    "    # resulting shapes: (B, Hq, L, D)\n",
    "    k_exp = k[:, mapping, :, :]\n",
    "    v_exp = v[:, mapping, :, :]\n",
    "\n",
    "    # Compute scaled dot-product scores: (B, Hq, L, L)\n",
    "    scale = 1.0 / math.sqrt(D)\n",
    "    # einsum is memory-efficient and clear here\n",
    "    scores = torch.einsum(\"b h l d, b h m d -> b h l m\", q, k_exp) * scale\n",
    "\n",
    "    # Build mask of allowed keys for each query position according to naive logic\n",
    "    # Query positions are rows (i), key positions are cols (j)\n",
    "    idx = torch.arange(Lq, device=device)\n",
    "    q_idx = idx[:, None]  # (L,1)\n",
    "    k_idx = idx[None, :]  # (1,L)\n",
    "\n",
    "    if window_size is None:\n",
    "        # window_size None means full window\n",
    "        if is_causal:\n",
    "            # causal only: allow j <= i\n",
    "            mask = (k_idx <= q_idx)  # (L,L)\n",
    "        else:\n",
    "            # full non-causal: allow all positions\n",
    "            mask = torch.ones((Lq, Lq), dtype=torch.bool, device=device)\n",
    "    else:\n",
    "        # sliding window with possible causal behavior\n",
    "        # start = max(0, i - window_size + 1)\n",
    "        start_idx = q_idx - (window_size - 1)\n",
    "        start_idx = torch.clamp(start_idx, min=0)\n",
    "        # allowed j satisfy: j >= start_idx and j < end\n",
    "        if is_causal:\n",
    "            # end = i+1\n",
    "            end_idx = q_idx + 1\n",
    "            mask = (k_idx >= start_idx) & (k_idx < end_idx)\n",
    "        else:\n",
    "            # non-causal: end = L (allow future keys too)\n",
    "            mask = (k_idx >= start_idx) & (k_idx < Lq)\n",
    "\n",
    "    # Expand mask to (1,1,L,L) to broadcast over B and Hq\n",
    "    mask = mask.view(1, 1, Lq, Lq)\n",
    "\n",
    "    # Apply mask by setting disallowed logits to a large negative number\n",
    "    scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "    # Softmax over key positions\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # Weighted sum to get output: (B, Hq, L, D)\n",
    "    out = torch.einsum(\"b h l m, b h m d -> b h l d\", weights, v_exp)\n",
    "\n",
    "    return out\n",
    "\n",
    "# small smoke test when run as script\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    B, Hq, Hkv, L, D = 1, 8, 2, 512, 16\n",
    "    q = torch.randn(B, Hq, L, D, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    k = torch.randn(B, Hkv, L, D, device=q.device)\n",
    "    v = torch.randn(B, Hkv, L, D, device=q.device)\n",
    "    out = flash_attention_forward(q, k, v, is_causal=True, window_size=128)\n",
    "    print(\"smoke test output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5ebfd",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Autograder for Problem 6: Sliding Window Attention\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "def repeat_kv(x, num_groups):\n",
    "    \"\"\"Helper function to repeat K/V heads for GQA naive implementation.\"\"\"\n",
    "    if num_groups == 1:\n",
    "        return x\n",
    "    B, H_kv, N, D = x.shape\n",
    "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
    "    return x.reshape(B, H_kv * num_groups, N, D)\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)   # (seq_len, 1)\n",
    "    col = idx.unsqueeze(0)   # (1, seq_len)\n",
    "\n",
    "    # 1) sliding window:  i - (window_size-1) <= j <= i\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "\n",
    "    # 2) sink at start:   j < sink_size  *and*  j <= i\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
    "    \"\"\"\n",
    "    A correct, robust PyTorch implementation of standard attention for comparison.\n",
    "    Supports GQA, Sliding Window, and Attention Sinks.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_heads_q, seq_len, head_dim = Q.shape\n",
    "    _, num_heads_kv, seq_len, head_dim = K.shape\n",
    "\n",
    "    if num_heads_q != num_heads_kv:\n",
    "        num_groups = num_heads_q // num_heads_kv\n",
    "        K = repeat_kv(K, num_groups)\n",
    "        V = repeat_kv(V, num_groups)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    S = (Q @ K.transpose(-1, -2)) * scale\n",
    "    \n",
    "    if is_causal:\n",
    "        mask = None\n",
    "        if window_size is None: # Causal only\n",
    "            mask = create_mask_bool(seq_len=seq_len, window_size=seq_len, sink_size=0, device=Q.device)\n",
    "        else:\n",
    "            if sink_size is None: # SWA only\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=0, device=Q.device)\n",
    "            else: # SWA + Sink\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=sink_size, device=Q.device)\n",
    "                \n",
    "        S.masked_fill_(~mask, -float('inf'))\n",
    "\n",
    "    P = torch.nn.functional.softmax(S, dim=-1, dtype=torch.float32).to(Q.dtype)\n",
    "    O_final = P @ V\n",
    "    L_final = torch.logsumexp(S.to(torch.float32), dim=-1)\n",
    "    \n",
    "    return O_final, L_final\n",
    "\n",
    "def run_correctness_test(test_case, triton_func, is_causal=False, is_gqa=False, is_swa=False, problem_num=1):\n",
    "    \"\"\"Run a single correctness test.\"\"\"\n",
    "    \n",
    "    window_size, sink_size = None, None\n",
    "    if is_gqa and not is_swa: # GQA only \n",
    "        batch, heads_q, heads_kv, seq_len, dim = test_case\n",
    "        config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}\"\n",
    "    elif is_swa: # GQA + SWA\n",
    "        batch, heads_q, heads_kv, seq_len, dim, *window_params = test_case\n",
    "        if len(window_params) == 1:\n",
    "            window_size = window_params[0]\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}\"\n",
    "        else:\n",
    "            window_size, sink_size = window_params\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}, S={sink_size}\"\n",
    "    else:\n",
    "        batch, heads_q, seq_len, dim = test_case\n",
    "        heads_kv = heads_q\n",
    "        config_str = f\"B={batch}, H={heads_q}, L={seq_len}, D={dim}\"\n",
    "\n",
    "    q = torch.randn(batch, heads_q, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    k = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    v = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    \n",
    "    try:\n",
    "        if is_swa:\n",
    "            if sink_size is not None:\n",
    "                triton_out = triton_func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "            else:\n",
    "                triton_out = triton_func(q, k, v, is_causal=is_causal, window_size=window_size)\n",
    "        else:\n",
    "            triton_out = triton_func(q, k, v, is_causal=is_causal)\n",
    "        \n",
    "        naive_out, _ = naive_attention(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        \n",
    "        if torch.allclose(triton_out, naive_out, atol=1e-2, rtol=1e-2):\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! ({config_str})\")\n",
    "            return True\n",
    "        else:\n",
    "            max_diff = torch.max(torch.abs(triton_out - naive_out)).item()\n",
    "            print(f\"❌ P{problem_num} Correctness Test Failed! ({config_str}) Max diff: {max_diff:.6f}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ P{problem_num} Error during execution ({config_str}): {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def benchmark_attention(triton_func, naive_func, test_params, is_causal, is_gqa=False, is_swa=False):\n",
    "    \"\"\"Utility to benchmark an attention function and compare it to a naive implementation.\"\"\"\n",
    "    print(\"\\n--- Running Performance Benchmark ---\")\n",
    "    window_size, sink_size = None, None\n",
    "    if is_gqa and not is_swa: # GQA only \n",
    "        batch, heads_q, heads_kv, seq_len, dim = test_params\n",
    "        config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}\"\n",
    "    elif is_swa: # GQA + SWA\n",
    "        batch, heads_q, heads_kv, seq_len, dim, *window_params = test_params\n",
    "        if len(window_params) == 1:\n",
    "            window_size = window_params[0]\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}\"\n",
    "        else:\n",
    "            window_size, sink_size = window_params\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}, S={sink_size}\"\n",
    "    else:\n",
    "        batch, heads_q, seq_len, dim = test_params\n",
    "        heads_kv = heads_q\n",
    "        config_str = f\"B={batch}, H={heads_q}, L={seq_len}, D={dim}\"\n",
    "\n",
    "    print(f\"Benchmark Config: {config_str}, Causal={is_causal}\")\n",
    "    \n",
    "    q = torch.randn(batch, heads_q, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    k = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    v = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "\n",
    "    def _run_benchmark(func, is_triton):\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):  # Warmup\n",
    "            if is_triton:\n",
    "                if is_swa:\n",
    "                    if sink_size is not None:\n",
    "                        _ = func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "                    else:\n",
    "                        _ = func(q, k, v, is_causal=is_causal, window_size=window_size)\n",
    "                else:\n",
    "                    _ = func(q, k, v, is_causal=is_causal)\n",
    "            else:\n",
    "                _, _ = func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(100):  # Actual benchmark\n",
    "            if is_triton:\n",
    "                if is_swa:\n",
    "                    if sink_size is not None:\n",
    "                        _ = func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "                    else:\n",
    "                        _ = func(q, k, v, is_causal=is_causal, window_size=window_size)\n",
    "                else:\n",
    "                    _ = func(q, k, v, is_causal=is_causal)\n",
    "            else:\n",
    "                _, _ = func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 100 * 1000  # Convert to ms\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e9  # Convert to GB\n",
    "        \n",
    "        return avg_time, peak_memory\n",
    "\n",
    "    # Benchmark both implementations\n",
    "    triton_time, triton_memory = _run_benchmark(triton_func, is_triton=True)\n",
    "    naive_time, naive_memory = _run_benchmark(naive_func, is_triton=False)\n",
    "\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"{'Implementation':<25} | {'Avg Time (ms)':<20} | {'Peak Memory (GB)':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'PyTorch (Naive)':<25} | {naive_time:<20.4f} | {naive_memory:<20.4f}\")\n",
    "    print(f\"{'Triton (Flash)':<25} | {triton_time:<20.4f} | {triton_memory:<20.4f}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    speedup = naive_time / triton_time\n",
    "    memory_reduction = naive_memory / triton_memory\n",
    "    \n",
    "    print(f\"Triton is {speedup:.2f}x faster than PyTorch (Naive).\")\n",
    "    print(f\"Triton uses {memory_reduction:.2f}x less memory.\")\n",
    "\n",
    "def check_problem_6():\n",
    "    \"\"\"Checks Problem 6: Sliding Window Attention.\"\"\"\n",
    "    problem_num = 6\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Sliding Window Attention ---\")\n",
    "    \n",
    "    torch.manual_seed(47)\n",
    "    # Test cases: (Batch, Heads_Q, Heads_KV, SeqLen, Dim, WindowSize)\n",
    "    window_size = 128\n",
    "    test_cases = [\n",
    "        (1, 8, 2, 512, 16, window_size),\n",
    "        (1, 8, 2, 1024, 16, window_size),\n",
    "        (1, 16, 2, 2048, 16, window_size),\n",
    "        (1, 16, 2, 4096, 16, window_size),\n",
    "    ]\n",
    "    \n",
    "    results = [run_correctness_test(case, flash_attention_forward, is_causal=True, is_gqa=True, is_swa=True, problem_num=problem_num) for case in test_cases]\n",
    "    if all(results):\n",
    "        print(f\"\\nAll P{problem_num} correctness tests passed!\")\n",
    "        benchmark_attention(flash_attention_forward, naive_attention, test_cases[-1], is_causal=True, is_gqa=True, is_swa=True)\n",
    "    else:\n",
    "        print(f\"\\n❌ Some P{problem_num} tests failed. Please check your implementation.\")\n",
    "\n",
    "# Run the autograder\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🚀 Starting Problem 6 Autograder...\")\n",
    "    print(\"📝 Testing: GQA + Sliding Window Attention + Causal Masking\")\n",
    "    check_problem_6()\n",
    "else:\n",
    "    print(\"❌ CUDA not available. Please run this on a GPU-enabled environment.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
