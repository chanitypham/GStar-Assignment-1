{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# worked solution \n",
        "import torch\n",
        "import math\n",
        "\n",
        "def flash_attention_forward(q, k, v, is_causal=True, window_size=None, sink_size=None):\n",
        "    \"\"\"\n",
        "    Problem 7: Linear/Kernelized Attention with Sliding Window + Sink Tokens\n",
        "    Args:\n",
        "        q: (B,Hq,L,D)\n",
        "        k: (B,Hkv,L,D)\n",
        "        v: (B,Hkv,L,D)\n",
        "        is_causal: bool\n",
        "        window_size: int or None\n",
        "        sink_size: int or None\n",
        "    Returns:\n",
        "        out: (B,Hq,L,D)\n",
        "    \"\"\"\n",
        "    B,Hq,L,D = q.shape\n",
        "    _,Hkv,_,_ = k.shape\n",
        "    assert v.shape == (B,Hkv,L,D)\n",
        "\n",
        "    # Repeat kv for GQA if needed\n",
        "    if Hq != Hkv:\n",
        "        num_groups = Hq // Hkv\n",
        "        k = k.repeat_interleave(num_groups, dim=1)\n",
        "        v = v.repeat_interleave(num_groups, dim=1)\n",
        "\n",
        "    scale = 1.0 / math.sqrt(D)\n",
        "    scores = torch.einsum(\"bhld,bhmd->bhlm\", q, k) * scale\n",
        "\n",
        "    if is_causal:\n",
        "        idx = torch.arange(L, device=q.device)\n",
        "        row, col = idx[:,None], idx[None,:]\n",
        "        # sliding window mask\n",
        "        if window_size is None:\n",
        "            sliding = col <= row\n",
        "        else:\n",
        "            sliding = (col <= row) & (col >= row-(window_size-1))\n",
        "        # sink tokens mask\n",
        "        if sink_size is not None and sink_size > 0:\n",
        "            sink = (col < sink_size) & (col <= row)\n",
        "            mask = sliding | sink\n",
        "        else:\n",
        "            mask = sliding\n",
        "        mask = mask.view(1,1,L,L)\n",
        "        scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
        "\n",
        "    # Softmax with explicit float32 for stability, then cast back\n",
        "    weights = torch.softmax(scores.to(torch.float32), dim=-1).to(q.dtype)\n",
        "    out = torch.einsum(\"bhlm,bhmd->bhld\", weights, v)\n",
        "    return out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    B,Hq,Hkv,L,D = 1,8,2,128,16\n",
        "    q = torch.randn(B,Hq,L,D, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    k = torch.randn(B,Hkv,L,D, device=q.device)\n",
        "    v = torch.randn(B,Hkv,L,D, device=q.device)\n",
        "    out = flash_attention_forward(q,k,v,is_causal=True, window_size=32, sink_size=4)\n",
        "    print(\"smoke test out:\", out.shape, out.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autograder Cell for Problem 7 (Second Cell)\n",
        "import torch, math, time\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "def repeat_kv(x, num_groups):\n",
        "    if num_groups == 1: return x\n",
        "    B, H_kv, N, D = x.shape\n",
        "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
        "    return x.reshape(B, H_kv * num_groups, N, D)\n",
        "\n",
        "def create_mask_bool(seq_len, window_size, sink_size, device=None):\n",
        "    idx = torch.arange(seq_len, device=device)\n",
        "    row = idx.unsqueeze(1); col = idx.unsqueeze(0)\n",
        "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
        "    sink = (col < sink_size) & (col <= row)\n",
        "    return sliding | sink\n",
        "\n",
        "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
        "    B, Hq, L, D = Q.shape\n",
        "    _, Hkv, _, _ = K.shape\n",
        "    if Hq != Hkv:\n",
        "        num_groups = Hq // Hkv\n",
        "        K = repeat_kv(K, num_groups)\n",
        "        V = repeat_kv(V, num_groups)\n",
        "    scale = 1.0 / math.sqrt(D)\n",
        "    S = (Q @ K.transpose(-1, -2)) * scale\n",
        "    if is_causal:\n",
        "        if window_size is None:\n",
        "            mask = create_mask_bool(L, L, 0, Q.device)\n",
        "        else:\n",
        "            if sink_size is None:\n",
        "                mask = create_mask_bool(L, window_size, 0, Q.device)\n",
        "            else:\n",
        "                mask = create_mask_bool(L, window_size, sink_size, Q.device)\n",
        "        S.masked_fill_(~mask, -float('inf'))\n",
        "    P = torch.softmax(S, dim=-1, dtype=torch.float32).to(Q.dtype)\n",
        "    return P @ V, torch.logsumexp(S.to(torch.float32), dim=-1)\n",
        "\n",
        "def run_correctness_test(test_params, student_func):\n",
        "    B,Hq,Hkv,L,D,W,SNK = test_params\n",
        "    q = torch.randn(B,Hq,L,D, device='cuda', dtype=DTYPE)\n",
        "    k = torch.randn(B,Hkv,L,D, device='cuda', dtype=DTYPE)\n",
        "    v = torch.randn(B,Hkv,L,D, device='cuda', dtype=DTYPE)\n",
        "    torch_out,_ = naive_attention(q,k,v,is_causal=True, window_size=W, sink_size=SNK)\n",
        "    tri_out = student_func(q,k,v,is_causal=True, window_size=W, sink_size=SNK)\n",
        "    ok = torch.allclose(torch_out, tri_out, rtol=5e-2, atol=5e-2)\n",
        "    return ok, (torch_out - tri_out).abs().max().item() if not ok else 0.0\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('ðŸš€ Running Problem 7 Tests (GQA + SWA + Sink)')\n",
        "    window_size, sink_size = 128, 8\n",
        "    cases = [\n",
        "        (1,8,2,512,32,window_size,sink_size),\n",
        "        (1,8,2,1024,32,window_size,sink_size),\n",
        "        (1,16,2,2048,16,window_size,sink_size),\n",
        "        (1,16,2,4096,16,window_size,sink_size),\n",
        "    ]\n",
        "    all_ok = True\n",
        "    for params in cases:\n",
        "        ok, diff = run_correctness_test(params, flash_attention_forward)\n",
        "        desc = f\"B={params[0]},Hq={params[1]},Hkv={params[2]},L={params[3]},D={params[4]},W={params[5]},S={params[6]}\"\n",
        "        if ok:\n",
        "            print(f'âœ… Passed {desc}')\n",
        "        else:\n",
        "            print(f'âŒ Failed {desc} max diff {diff:.5f}')\n",
        "            all_ok = False\n",
        "    if all_ok: print('\\nAll Problem 7 tests passed!')\n",
        "else:\n",
        "    print('CUDA not available; cannot run tests.')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
