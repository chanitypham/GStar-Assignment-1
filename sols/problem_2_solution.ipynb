{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3f7b71",
   "metadata": {},
   "source": [
    "# GStar Assignment Problem 2: Triton Weighted Row Sum\n",
    "\n",
    "This notebook implements **Stage 2** of the GStar bootcamp: writing your first GPU kernel with weighted row sums using Triton.\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "Write a Triton kernel to compute the weighted sum of each row in a matrix:\n",
    "\n",
    "```\n",
    "Y[i] = sum_{j=0}^{N_COLS-1} X[i, j] * W[j]\n",
    "```\n",
    "\n",
    "This introduces key concepts:\n",
    "\n",
    "- Parallel thinking and memory access patterns\n",
    "- Block-based efficiency in GPU kernels\n",
    "- Triton JIT compilation and optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8f8db",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import torch, triton, and other necessary libraries for implementing the weighted row sum kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA capability: {torch.cuda.get_device_capability()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4080b37",
   "metadata": {},
   "source": [
    "## Section 2: Implement Weighted Row Sum Kernel\n",
    "\n",
    "Complete the weighted_row_sum_kernel function using Triton JIT compilation, including memory access patterns, block-based computation, and the forward pass function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2beb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def weighted_row_sum_kernel(\n",
    "    X_ptr,        # Pointer to the input tensor\n",
    "    W_ptr,        # Pointer to the weight vector\n",
    "    Y_ptr,        # Pointer to the output vector\n",
    "    N_COLS,       # Number of columns in the input tensor\n",
    "    BLOCK_SIZE: tl.constexpr  # Block size for the kernel\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel to compute the weighted sum of each row in a matrix.\n",
    "    Y[i] = sum_{j=0}^{N_COLS-1} X[i, j] * W[j]\n",
    "    \"\"\"\n",
    "    # 1. Get the row index for the current program instance.\n",
    "    #    Hint: Use tl.program_id(axis=0).\n",
    "    row_idx = tl.program_id(axis=0)\n",
    "\n",
    "    # 2. Create a pointer to the start of the current row in the input tensor X.\n",
    "    #    Hint: The offset depends on the row index and the number of columns (N_COLS).\n",
    "    row_start_ptr = X_ptr + row_idx * N_COLS\n",
    "    \n",
    "    # 3. Create a pointer for the output vector Y.\n",
    "    output_ptr = Y_ptr + row_idx\n",
    "\n",
    "    # 4. Initialize an accumulator for the sum of the products for a block.\n",
    "    #    This should be a block-sized tensor of zeros.\n",
    "    #    Hint: Use tl.zeros with shape (BLOCK_SIZE,) and dtype tl.float32.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    # 5. Iterate over the columns of the row in blocks of BLOCK_SIZE.\n",
    "    #    Hint: Use a for loop with tl.cdiv(N_COLS, BLOCK_SIZE).\n",
    "    for col_block_start in range(0, tl.cdiv(N_COLS, BLOCK_SIZE) * BLOCK_SIZE, BLOCK_SIZE):\n",
    "        # - Calculate the offsets for the current block of columns.\n",
    "        #   Hint: Start from the block's beginning and add tl.arange(0, BLOCK_SIZE).\n",
    "        col_offsets = col_block_start + tl.arange(0, BLOCK_SIZE)\n",
    "        \n",
    "        # - Create a mask to prevent out-of-bounds memory access for the last block.\n",
    "        #   Hint: Compare col_offsets with N_COLS.\n",
    "        mask = col_offsets < N_COLS\n",
    "        \n",
    "        # - Load a block of data from X and W safely using the mask.\n",
    "        #   Hint: Use tl.load with the appropriate pointers, offsets, and mask.\n",
    "        #   Use `other=0.0` to handle out-of-bounds elements.\n",
    "        x_chunk = tl.load(row_start_ptr + col_offsets, mask=mask, other=0.0)\n",
    "        w_chunk = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)\n",
    "        \n",
    "        # - Compute the element-wise product and add it to the accumulator.\n",
    "        accumulator += x_chunk * w_chunk\n",
    "        \n",
    "    # 6. Reduce the block-sized accumulator to a single scalar value after the loop.\n",
    "    #    Hint: Use tl.sum().\n",
    "    final_sum = tl.sum(accumulator)\n",
    "\n",
    "    # 7. Store the final accumulated sum to the output tensor Y.\n",
    "    #    Hint: Use tl.store().\n",
    "    tl.store(output_ptr, final_sum)\n",
    "    \n",
    "# --- END OF STUDENT IMPLEMENTATION ---\n",
    "\n",
    "\n",
    "def weighted_row_sum_forward(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Forward pass for the weighted row-sum operation using the Triton kernel.\n",
    "    \"\"\"\n",
    "    assert x.dim() == 2, \"Input tensor must be a 2D matrix\"\n",
    "    assert w.dim() == 1, \"Weight tensor must be a 1D vector\"\n",
    "    assert x.shape[1] == w.shape[0], \"Inner dimensions must match\"\n",
    "    assert x.is_cuda and w.is_cuda, \"Tensors must be on CUDA\"\n",
    "    \n",
    "    N_ROWS, N_COLS = x.shape\n",
    "    \n",
    "    # The output is a 1D tensor with length equal to the number of rows.\n",
    "    y = torch.empty(N_ROWS, device=x.device, dtype=torch.float32)\n",
    "    \n",
    "    # The grid is 1D, with one program instance per row.\n",
    "    grid = (N_ROWS,)\n",
    "    \n",
    "    # Block size is a power of 2. 1024 is a good default.\n",
    "    BLOCK_SIZE = 1024\n",
    "    \n",
    "    # Launch the kernel\n",
    "    weighted_row_sum_kernel[grid](\n",
    "        x, w, y,\n",
    "        N_COLS=N_COLS,\n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    \n",
    "    return y.to(x.dtype) # Cast back to original dtype\n",
    "\n",
    "def torch_weighted_row_sum(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reference implementation using pure PyTorch.\n",
    "    \"\"\"\n",
    "    return (x * w).sum(dim=1)\n",
    "\n",
    "print(\"✅ Weighted row sum kernel implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2508f",
   "metadata": {},
   "source": [
    "## Section 3: Test Implementation with Autograder\n",
    "\n",
    "Run the autograder code to validate correctness and performance of the weighted row sum implementation against reference implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data type for testing\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "def repeat_kv(x, num_groups):\n",
    "    \"\"\"Helper function to repeat K/V heads for GQA naive implementation.\"\"\"\n",
    "    if num_groups == 1:\n",
    "        return x\n",
    "    B, H_kv, N, D = x.shape\n",
    "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
    "    return x.reshape(B, H_kv * num_groups, N, D)\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)   # (seq_len, 1)\n",
    "    col = idx.unsqueeze(0)   # (1, seq_len)\n",
    "\n",
    "    # 1) sliding window:  i - (window_size-1) <= j <= i\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "\n",
    "    # 2) sink at start:   j < sink_size  *and*  j <= i\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
    "    \"\"\"\n",
    "    A correct, robust PyTorch implementation of standard attention for comparison.\n",
    "    Supports GQA, Sliding Window, and Attention Sinks.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_heads_q, seq_len, head_dim = Q.shape\n",
    "    _, num_heads_kv, seq_len, head_dim = K.shape\n",
    "\n",
    "    if num_heads_q != num_heads_kv:\n",
    "        num_groups = num_heads_q // num_heads_kv\n",
    "        K = repeat_kv(K, num_groups)\n",
    "        V = repeat_kv(V, num_groups)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "\n",
    "    if window_size is not None:\n",
    "        assert sink_size is not None, \"Must specify sink_size when using window_size\"\n",
    "        mask_bool = create_mask_bool(seq_len, window_size, sink_size, device=scores.device)\n",
    "        scores = scores.masked_fill(~mask_bool.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    elif is_causal:\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device, dtype=torch.bool))\n",
    "        scores = scores.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "    L = torch.logsumexp(scores, dim=-1, keepdim=False)\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    O = torch.matmul(P, V)\n",
    "    return O, L\n",
    "\n",
    "def check_problem_1():\n",
    "    \"\"\"Checks Problem 1: PyTorch Tiled Attention.\"\"\"\n",
    "    problem_num = 1\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Tiled Flash Attention ---\")\n",
    "    try:\n",
    "        # Note: For this notebook, we're only testing problem 2, so we'll skip problem 1\n",
    "        print(\"Problem 1 implementation not included in this notebook. Skipping...\")\n",
    "        return\n",
    "    except ImportError:\n",
    "        print(f\"Could not import FlashAttention2Function from solution_{problem_num}.py.\")\n",
    "        return\n",
    "\n",
    "def check_problem_2():\n",
    "    \"\"\"Checks Problem 2: Triton Weighted Row-Sum.\"\"\"\n",
    "    problem_num = 2\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Triton Weighted Row-Sum ---\")\n",
    "        \n",
    "    torch.manual_seed(43)\n",
    "    test_cases = [(512, 1024), (1024, 4096), (2048, 8192), (4096, 8192)]\n",
    "    \n",
    "    def run_p2_test(rows, cols):\n",
    "        x = torch.randn(rows, cols, device='cuda', dtype=DTYPE)\n",
    "        w = torch.randn(cols, device='cuda', dtype=DTYPE)\n",
    "        torch_result = torch_weighted_row_sum(x, w)\n",
    "        triton_result = weighted_row_sum_forward(x, w)\n",
    "        \n",
    "        param_str = f\"(Rows={rows}, Cols={cols})\"\n",
    "        if torch.allclose(torch_result, triton_result, rtol=5e-2, atol=5e-2):\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! {param_str}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ P{problem_num} Correctness Test Failed! {param_str}\")\n",
    "            print(f\" Max diff: {(torch_result - triton_result).abs().max()}\")\n",
    "            return False\n",
    "            \n",
    "    results = [run_p2_test(*case) for case in test_cases]\n",
    "    if all(results): \n",
    "        print(f\"\\nAll P{problem_num} correctness tests passed!\")\n",
    "    else:\n",
    "        print(f\"\\nSome P{problem_num} tests failed. Please check your implementation.\")\n",
    "\n",
    "# Run the autograder for Problem 2\n",
    "check_problem_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f2663",
   "metadata": {},
   "source": [
    "## Test Your Implementation\n",
    "\n",
    "Run a quick test to see your Triton kernel in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick functionality test\n",
    "print(\"\\n--- Quick Functionality Test ---\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create test tensors\n",
    "rows, cols = 4, 8\n",
    "x = torch.randn(rows, cols, device='cuda', dtype=torch.float32)\n",
    "w = torch.randn(cols, device='cuda', dtype=torch.float32)\n",
    "\n",
    "print(f\"Input matrix X shape: {x.shape}\")\n",
    "print(f\"Weight vector W shape: {w.shape}\")\n",
    "print(f\"\\nX =\\n{x}\")\n",
    "print(f\"\\nW = {w}\")\n",
    "\n",
    "# Compute results\n",
    "torch_result = torch_weighted_row_sum(x, w)\n",
    "triton_result = weighted_row_sum_forward(x, w)\n",
    "\n",
    "print(f\"\\nPyTorch result: {torch_result}\")\n",
    "print(f\"Triton result:  {triton_result}\")\n",
    "print(f\"Max difference: {torch.abs(torch_result - triton_result).max().item():.2e}\")\n",
    "print(f\"Results match: {torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085f03b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "🎉 **Congratulations!** You've successfully implemented your first GPU kernel using Triton!\n",
    "\n",
    "### Key Concepts Learned:\n",
    "\n",
    "1. **Parallel Programming**: Each program instance handles one row independently\n",
    "2. **Memory Access Patterns**: Efficient block-based loading with proper masking\n",
    "3. **Triton JIT Compilation**: Writing GPU kernels in Python with automatic optimization\n",
    "4. **Block-based Computation**: Processing data in chunks for better memory locality\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "- **Problem 3**: Implement Flash Attention forward pass using Triton\n",
    "- **Problem 4**: Add causal masking for language model applications\n",
    "- **Advanced Problems**: Grouped Query Attention, Sliding Window, and more!\n",
    "\n",
    "This foundation in parallel GPU computing will be essential for implementing the more complex Flash Attention algorithms in the upcoming stages.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
