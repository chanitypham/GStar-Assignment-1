{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848bb574",
   "metadata": {},
   "source": [
    "# Problem 4: Causal Flash Attention\n",
    "\n",
    "Implementation of FlashAttention-2 forward pass using Triton for causal attention.\n",
    "\n",
    "This implementation uses a two-phase approach:\n",
    "\n",
    "1. **Off-diagonal blocks**: Process blocks where all queries have indices > all keys (no masking needed)\n",
    "2. **Diagonal blocks**: Process blocks where queries and keys can overlap (causal masking required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e41c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "@triton.jit\n",
    "def _flash_attention_forward_causal_kernel(\n",
    "    # Pointers to Tensors\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr,\n",
    "    # Stride information for tensors\n",
    "    q_stride_b, q_stride_h, q_stride_s,\n",
    "    k_stride_b, k_stride_h, k_stride_s,\n",
    "    v_stride_b, v_stride_h, v_stride_s,\n",
    "    # Kernel parameters\n",
    "    softmax_scale,\n",
    "    SEQ_LEN,\n",
    "    N_HEADS,\n",
    "    # Constexpr tile sizes\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel for the forward pass of causal FlashAttention.\n",
    "    This is a template for student implementation.\n",
    "    \"\"\"\n",
    "    # 1. Identify the block of queries and the batch/head to be processed.\n",
    "    q_block_idx = tl.program_id(axis=0)\n",
    "    batch_head_idx = tl.program_id(axis=1)\n",
    "    \n",
    "    batch_idx = batch_head_idx // N_HEADS\n",
    "    head_idx = batch_head_idx % N_HEADS\n",
    "\n",
    "    # 2. Initialize accumulators in SRAM.\n",
    "    m_i = tl.full([BLOCK_M], -float('inf'), dtype=tl.float32)\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # 3. Load the block of queries (Q_i).\n",
    "    q_offsets = (q_block_idx * BLOCK_M + tl.arange(0, BLOCK_M))\n",
    "    q_ptrs = Q_ptr + batch_idx * q_stride_b + head_idx * q_stride_h + \\\n",
    "             (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    q_block = tl.load(q_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "    q_block = q_block.to(tl.float32)  # Convert to float32 for computation\n",
    "    \n",
    "    # PyTorch softmax is exp(x), Triton is exp2(x * log2(e)), log2(e) is approx 1.44269504\n",
    "    qk_scale = softmax_scale * 1.44269504\n",
    "\n",
    "    # --- Phase 1: Accumulate in Off-Diagonal Blocks (No Masking) ---\n",
    "    # Process key/value blocks that are strictly in the past (q_idx > k_idx).\n",
    "    for start_n in range(0, q_block_idx * BLOCK_M, BLOCK_N):\n",
    "        # Load K_j\n",
    "        k_offsets = start_n + tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K_ptr + batch_idx * k_stride_b + head_idx * k_stride_h + \\\n",
    "                 (k_offsets[None, :] * k_stride_s + tl.arange(0, HEAD_DIM)[:, None])\n",
    "        k_block = tl.load(k_ptrs, mask=k_offsets[None, :] < SEQ_LEN, other=0.0)\n",
    "        k_block = k_block.to(tl.float32)\n",
    "        \n",
    "        # Compute attention scores S_ij = Q_i * K_j^T\n",
    "        s_ij = tl.dot(q_block, k_block)\n",
    "        s_ij *= qk_scale\n",
    "        \n",
    "        # Load V_j\n",
    "        v_ptrs = V_ptr + batch_idx * v_stride_b + head_idx * v_stride_h + \\\n",
    "                 (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        v_block = v_block.to(tl.float32)\n",
    "        \n",
    "        # Online softmax update (same as non-causal case)\n",
    "        m_ij = tl.max(s_ij, 1)  # Row-wise max of current scores\n",
    "        m_new = tl.maximum(m_i, m_ij)  # Element-wise max with previous running max\n",
    "        \n",
    "        alpha = tl.exp2(m_i - m_new)  # Rescaling factor for previous values\n",
    "        beta = tl.exp2(m_ij - m_new)  # Rescaling factor for current values\n",
    "        \n",
    "        # Rescale previous accumulator and denominator\n",
    "        acc = acc * alpha[:, None]\n",
    "        l_i = l_i * alpha\n",
    "        \n",
    "        # Compute probabilities and update accumulator\n",
    "        p_ij = tl.exp2(s_ij - m_new[:, None])\n",
    "        acc += tl.dot(p_ij, v_block)\n",
    "        l_i += tl.sum(p_ij, 1)\n",
    "        \n",
    "        # Update running maximum\n",
    "        m_i = m_new\n",
    "\n",
    "\n",
    "    # --- Phase 2: Run on the Diagonal Blocks (With Masking) ---\n",
    "    # Process the blocks where query and key indices can overlap.\n",
    "    diag_start = q_block_idx * BLOCK_M\n",
    "    for start_n in range(diag_start, (q_block_idx + 1) * BLOCK_M, BLOCK_N):\n",
    "        # Load K_j\n",
    "        k_offsets = start_n + tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K_ptr + batch_idx * k_stride_b + head_idx * k_stride_h + \\\n",
    "                 (k_offsets[None, :] * k_stride_s + tl.arange(0, HEAD_DIM)[:, None])\n",
    "        k_block = tl.load(k_ptrs, mask=k_offsets[None, :] < SEQ_LEN, other=0.0)\n",
    "        k_block = k_block.to(tl.float32)\n",
    "        \n",
    "        # Compute attention scores S_ij = Q_i * K_j^T\n",
    "        s_ij = tl.dot(q_block, k_block)\n",
    "        s_ij *= qk_scale\n",
    "        \n",
    "        # Apply causal mask: q_idx >= k_idx\n",
    "        # Create causal mask where s_ij[i, j] = -inf if q_offsets[i] < k_offsets[j]\n",
    "        causal_mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "        s_ij = tl.where(causal_mask, s_ij, -float('inf'))\n",
    "        \n",
    "        # Load V_j\n",
    "        v_ptrs = V_ptr + batch_idx * v_stride_b + head_idx * v_stride_h + \\\n",
    "                 (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        v_block = v_block.to(tl.float32)\n",
    "        \n",
    "        # Online softmax update with masked scores\n",
    "        m_ij = tl.max(s_ij, 1)  # Row-wise max of current scores\n",
    "        m_new = tl.maximum(m_i, m_ij)  # Element-wise max with previous running max\n",
    "        \n",
    "        alpha = tl.exp2(m_i - m_new)  # Rescaling factor for previous values\n",
    "        beta = tl.exp2(m_ij - m_new)  # Rescaling factor for current values\n",
    "        \n",
    "        # Rescale previous accumulator and denominator\n",
    "        acc = acc * alpha[:, None]\n",
    "        l_i = l_i * alpha\n",
    "        \n",
    "        # Compute probabilities and update accumulator\n",
    "        p_ij = tl.exp2(s_ij - m_new[:, None])\n",
    "        acc += tl.dot(p_ij, v_block)\n",
    "        l_i += tl.sum(p_ij, 1)\n",
    "        \n",
    "        # Update running maximum\n",
    "        m_i = m_new\n",
    "\n",
    "\n",
    "    # 4. Normalize and write the final output block.\n",
    "    l_i_safe = l_i[:, None] + 1e-6\n",
    "    acc = acc / l_i_safe\n",
    "    \n",
    "    o_ptrs = O_ptr + batch_idx * q_stride_b + head_idx * q_stride_h + \\\n",
    "             (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "             \n",
    "    tl.store(o_ptrs, acc.to(O_ptr.dtype.element_ty), mask=q_offsets[:, None] < SEQ_LEN)\n",
    "\n",
    "def flash_attention_forward(q, k, v, is_causal=True):\n",
    "    \"\"\"\n",
    "    Python wrapper for the single-kernel, two-phase causal FlashAttention.\n",
    "    \"\"\"\n",
    "    if not is_causal:\n",
    "        raise NotImplementedError(\"This implementation is for causal attention. Use solution_3 for non-causal.\")\n",
    "\n",
    "    batch, n_heads, seq_len, head_dim = q.shape\n",
    "    o = torch.empty_like(q)\n",
    "    softmax_scale = 1.0 / math.sqrt(head_dim)\n",
    "    \n",
    "    BLOCK_M, BLOCK_N = 128, 64\n",
    "    grid = (triton.cdiv(seq_len, BLOCK_M), batch * n_heads)\n",
    "\n",
    "    _flash_attention_forward_causal_kernel[grid](\n",
    "        q, k, v, o,\n",
    "        q.stride(0), q.stride(1), q.stride(2),\n",
    "        k.stride(0), k.stride(1), k.stride(2),\n",
    "        v.stride(0), v.stride(1), v.stride(2),\n",
    "        softmax_scale,\n",
    "        seq_len,\n",
    "        n_heads,\n",
    "        HEAD_DIM=head_dim,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "    )\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0695b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder for Problem 4: Causal Flash Attention\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "def repeat_kv(x, num_groups):\n",
    "    \"\"\"Helper function to repeat K/V heads for GQA naive implementation.\"\"\"\n",
    "    if num_groups == 1:\n",
    "        return x\n",
    "    B, H_kv, N, D = x.shape\n",
    "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
    "    return x.reshape(B, H_kv * num_groups, N, D)\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)   # (seq_len, 1)\n",
    "    col = idx.unsqueeze(0)   # (1, seq_len)\n",
    "\n",
    "    # 1) sliding window:  i - (window_size-1) <= j <= i\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "\n",
    "    # 2) sink at start:   j < sink_size  *and*  j <= i\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
    "    \"\"\"\n",
    "    A correct, robust PyTorch implementation of standard attention for comparison.\n",
    "    Supports GQA, Sliding Window, and Attention Sinks.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_heads_q, seq_len, head_dim = Q.shape\n",
    "    _, num_heads_kv, seq_len, head_dim = K.shape\n",
    "\n",
    "    if num_heads_q != num_heads_kv:\n",
    "        num_groups = num_heads_q // num_heads_kv\n",
    "        K = repeat_kv(K, num_groups)\n",
    "        V = repeat_kv(V, num_groups)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    S = (Q @ K.transpose(-1, -2)) * scale\n",
    "    \n",
    "    if is_causal:\n",
    "        mask = None\n",
    "        if window_size is None: # Causal only\n",
    "            mask = create_mask_bool(seq_len=seq_len, window_size=seq_len, sink_size=0, device=Q.device)\n",
    "        else:\n",
    "            if sink_size is None: # SWA only\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=0, device=Q.device)\n",
    "            else: # SWA + Sink\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=sink_size, device=Q.device)\n",
    "                \n",
    "        S.masked_fill_(~mask, -float('inf'))\n",
    "\n",
    "    return F.softmax(S, dim=-1, dtype=torch.float32).to(Q.dtype) @ V\n",
    "\n",
    "def run_correctness_test(test_case, student_fn, is_causal=False, is_gqa=False, is_swa=False, problem_num=None):\n",
    "    \"\"\"Run a single correctness test case.\"\"\"\n",
    "    if len(test_case) == 4:\n",
    "        batch, n_heads, seq_len, head_dim = test_case\n",
    "        n_heads_kv = n_heads\n",
    "        window_size, sink_size = None, None\n",
    "    elif len(test_case) == 5:\n",
    "        batch, n_heads, n_heads_kv, seq_len, head_dim = test_case\n",
    "        window_size, sink_size = None, None\n",
    "    elif len(test_case) == 7:\n",
    "        batch, n_heads, n_heads_kv, seq_len, head_dim, window_size, sink_size = test_case\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid test case format: {test_case}\")\n",
    "\n",
    "    Q = torch.randn(batch, n_heads, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "    K = torch.randn(batch, n_heads_kv, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "    V = torch.randn(batch, n_heads_kv, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "\n",
    "    expected = naive_attention(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    \n",
    "    try:\n",
    "        if is_gqa and is_swa:\n",
    "            actual = student_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        elif is_gqa:\n",
    "            actual = student_fn(Q, K, V, is_causal=is_causal)\n",
    "        elif is_swa:\n",
    "            actual = student_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        else:\n",
    "            actual = student_fn(Q, K, V, is_causal=is_causal)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ P{problem_num} Test Failed! (B={batch}, H={n_heads}, L={seq_len}, D={head_dim}) - Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    if torch.allclose(actual, expected, atol=1e-2, rtol=1e-2):\n",
    "        if len(test_case) == 4:\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! (B={batch}, H={n_heads}, L={seq_len}, D={head_dim})\")\n",
    "        elif len(test_case) == 5:\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! (B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim})\")\n",
    "        elif len(test_case) == 7:\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! (B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim}, W={window_size}, S={sink_size})\")\n",
    "        return True\n",
    "    else:\n",
    "        if len(test_case) == 4:\n",
    "            print(f\"❌ P{problem_num} Test Failed! (B={batch}, H={n_heads}, L={seq_len}, D={head_dim}) - Results do not match.\")\n",
    "        elif len(test_case) == 5:\n",
    "            print(f\"❌ P{problem_num} Test Failed! (B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim}) - Results do not match.\")\n",
    "        elif len(test_case) == 7:\n",
    "            print(f\"❌ P{problem_num} Test Failed! (B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim}, W={window_size}, S={sink_size}) - Results do not match.\")\n",
    "        print(f\"Max difference: {torch.max(torch.abs(actual - expected)).item():.6f}\")\n",
    "        return False\n",
    "\n",
    "def benchmark_attention(triton_fn, pytorch_fn, test_case, is_causal=False, is_gqa=False, is_swa=False):\n",
    "    \"\"\"Benchmark the Triton implementation against PyTorch.\"\"\"\n",
    "    if len(test_case) == 4:\n",
    "        batch, n_heads, seq_len, head_dim = test_case\n",
    "        n_heads_kv = n_heads\n",
    "        window_size, sink_size = None, None\n",
    "    elif len(test_case) == 5:\n",
    "        batch, n_heads, n_heads_kv, seq_len, head_dim = test_case\n",
    "        window_size, sink_size = None, None\n",
    "    elif len(test_case) == 7:\n",
    "        batch, n_heads, n_heads_kv, seq_len, head_dim, window_size, sink_size = test_case\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid test case format: {test_case}\")\n",
    "\n",
    "    print(\"\\n--- Running Performance Benchmark ---\")\n",
    "    if len(test_case) == 4:\n",
    "        print(f\"Benchmark Config: B={batch}, H={n_heads}, L={seq_len}, D={head_dim}, Causal={is_causal}\")\n",
    "    elif len(test_case) == 5:\n",
    "        print(f\"Benchmark Config: B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim}, Causal={is_causal}\")\n",
    "    elif len(test_case) == 7:\n",
    "        print(f\"Benchmark Config: B={batch}, H_Q={n_heads}, H_KV={n_heads_kv}, L={seq_len}, D={head_dim}, W={window_size}, S={sink_size}, Causal={is_causal}\")\n",
    "\n",
    "    Q = torch.randn(batch, n_heads, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "    K = torch.randn(batch, n_heads_kv, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "    V = torch.randn(batch, n_heads_kv, seq_len, head_dim, dtype=DTYPE, device='cuda')\n",
    "\n",
    "    # Warm up\n",
    "    for _ in range(10):\n",
    "        if is_gqa and is_swa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        elif is_gqa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "        elif is_swa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        else:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "        _ = pytorch_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark Triton\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    triton_times = []\n",
    "    for _ in range(100):\n",
    "        start_event.record()\n",
    "        if is_gqa and is_swa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        elif is_gqa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "        elif is_swa:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        else:\n",
    "            _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        triton_times.append(start_event.elapsed_time(end_event))\n",
    "\n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(100):\n",
    "        start_event.record()\n",
    "        _ = pytorch_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start_event.elapsed_time(end_event))\n",
    "\n",
    "    triton_avg = sum(triton_times) / len(triton_times)\n",
    "    pytorch_avg = sum(pytorch_times) / len(pytorch_times)\n",
    "\n",
    "    # Memory usage\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    if is_gqa and is_swa:\n",
    "        _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    elif is_gqa:\n",
    "        _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "    elif is_swa:\n",
    "        _ = triton_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    else:\n",
    "        _ = triton_fn(Q, K, V, is_causal=is_causal)\n",
    "    triton_memory = torch.cuda.max_memory_allocated() / (1024**3)  # Convert to GB\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    _ = pytorch_fn(Q, K, V, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    pytorch_memory = torch.cuda.max_memory_allocated() / (1024**3)  # Convert to GB\n",
    "\n",
    "    speedup = pytorch_avg / triton_avg\n",
    "    memory_reduction = pytorch_memory / triton_memory\n",
    "\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"{'Implementation':<25} | {'Avg Time (ms)':<20} | {'Peak Memory (GB)':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'PyTorch (Naive)':<25} | {pytorch_avg:<20.4f} | {pytorch_memory:<20.4f}\")\n",
    "    print(f\"{'Triton (Flash)':<25} | {triton_avg:<20.4f} | {triton_memory:<20.4f}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Triton is {speedup:.2f}x faster than PyTorch (Naive).\")\n",
    "    print(f\"Triton uses {memory_reduction:.2f}x less memory.\")\n",
    "\n",
    "def check_problem_4():\n",
    "    \"\"\"Checks Problem 4: Causal Flash Attention.\"\"\"\n",
    "    problem_num = 4\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Causal Flash Attention ---\")\n",
    "    \n",
    "    torch.manual_seed(45)\n",
    "    test_cases = [\n",
    "        (1, 8, 512, 16),\n",
    "        (1, 8, 1024, 16),\n",
    "        (1, 16, 2048, 16),\n",
    "        (1, 16, 4096, 16),\n",
    "    ]\n",
    "    \n",
    "    results = [run_correctness_test(case, flash_attention_forward, is_causal=True, is_gqa=False, is_swa=False, problem_num=problem_num) for case in test_cases]\n",
    "    if all(results):\n",
    "        print(f\"\\nAll P{problem_num} correctness tests passed!\")\n",
    "        benchmark_attention(flash_attention_forward, naive_attention, test_cases[-1], is_causal=True, is_gqa=False)\n",
    "\n",
    "# Run the autograder\n",
    "check_problem_4()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
