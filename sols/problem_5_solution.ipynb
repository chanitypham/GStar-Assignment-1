{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a498d2",
   "metadata": {},
   "source": [
    "# Problem 5: Grouped-Query Attention (GQA) Flash Attention\n",
    "\n",
    "Implementation of FlashAttention-2 forward pass using Triton for causal attention with Grouped-Query Attention (GQA).\n",
    "\n",
    "In GQA, multiple query heads share the same key and value heads to reduce memory and computation. For example, if we have 8 query heads and 2 key/value heads, then:\n",
    "\n",
    "- Query heads 0-3 use key/value head 0\n",
    "- Query heads 4-7 use key/value head 1\n",
    "\n",
    "This requires careful mapping from query head indices to key/value head indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "\n",
    "@triton.jit\n",
    "def _flash_attention_forward_gqa_kernel(\n",
    "    # Pointers to Tensors\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr,\n",
    "    # Stride information for tensors\n",
    "    q_stride_b, q_stride_h, q_stride_s,\n",
    "    k_stride_b, k_stride_h, k_stride_s,\n",
    "    v_stride_b, v_stride_h, v_stride_s,\n",
    "    # Kernel parameters\n",
    "    softmax_scale,\n",
    "    SEQ_LEN,\n",
    "    N_Q_HEADS,\n",
    "    N_KV_HEADS,\n",
    "    # Constexpr tile sizes\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel for the forward pass of causal FlashAttention with GQA.\n",
    "    \"\"\"\n",
    "    # 1. Identify the block of queries and the batch/head to be processed.\n",
    "    q_block_idx = tl.program_id(axis=0)\n",
    "    batch_head_idx = tl.program_id(axis=1)\n",
    "    \n",
    "    batch_idx = batch_head_idx // N_Q_HEADS\n",
    "    q_head_idx = batch_head_idx % N_Q_HEADS\n",
    "\n",
    "    # --- STUDENT IMPLEMENTATION: Part 1 - GQA Head Mapping ---\n",
    "    # Calculate how many query heads are in each group\n",
    "    heads_per_group = N_Q_HEADS // N_KV_HEADS\n",
    "    \n",
    "    # Use integer division to find the correct kv_head_idx\n",
    "    kv_head_idx = q_head_idx // heads_per_group\n",
    "    # --- END OF STUDENT IMPLEMENTATION ---\n",
    "\n",
    "    # 2. Initialize accumulators in SRAM.\n",
    "    m_i = tl.full([BLOCK_M], -float('inf'), dtype=tl.float32)\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # 3. Load the block of queries (Q_i).\n",
    "    q_offsets = (q_block_idx * BLOCK_M + tl.arange(0, BLOCK_M))\n",
    "    q_ptrs = Q_ptr + batch_idx * q_stride_b + q_head_idx * q_stride_h + \\\n",
    "             (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    q_block = tl.load(q_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "    q_block = q_block.to(tl.float32)  # Convert to float32 for computation\n",
    "    \n",
    "    qk_scale = softmax_scale * 1.44269504\n",
    "    \n",
    "    # --- Phase 1: Off-Diagonal Blocks ---\n",
    "    for start_n in range(0, q_block_idx * BLOCK_M, BLOCK_N):\n",
    "        # --- STUDENT IMPLEMENTATION: Part 2 - Off-diagonal blocks with GQA ---\n",
    "        # Load K_j using kv_head_idx\n",
    "        k_offsets = start_n + tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K_ptr + batch_idx * k_stride_b + kv_head_idx * k_stride_h + \\\n",
    "                 (k_offsets[None, :] * k_stride_s + tl.arange(0, HEAD_DIM)[:, None])\n",
    "        k_block = tl.load(k_ptrs, mask=k_offsets[None, :] < SEQ_LEN, other=0.0)\n",
    "        k_block = k_block.to(tl.float32)\n",
    "        \n",
    "        # Compute attention scores S_ij = Q_i * K_j^T\n",
    "        s_ij = tl.dot(q_block, k_block)\n",
    "        s_ij *= qk_scale\n",
    "        \n",
    "        # Load V_j using kv_head_idx\n",
    "        v_ptrs = V_ptr + batch_idx * v_stride_b + kv_head_idx * v_stride_h + \\\n",
    "                 (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        v_block = v_block.to(tl.float32)\n",
    "        \n",
    "        # Online softmax update (same as Problem 4)\n",
    "        m_ij = tl.max(s_ij, 1)  # Row-wise max of current scores\n",
    "        m_new = tl.maximum(m_i, m_ij)  # Element-wise max with previous running max\n",
    "        \n",
    "        alpha = tl.exp2(m_i - m_new)  # Rescaling factor for previous values\n",
    "        beta = tl.exp2(m_ij - m_new)  # Rescaling factor for current values\n",
    "        \n",
    "        # Rescale previous accumulator and denominator\n",
    "        acc = acc * alpha[:, None]\n",
    "        l_i = l_i * alpha\n",
    "        \n",
    "        # Compute probabilities and update accumulator\n",
    "        p_ij = tl.exp2(s_ij - m_new[:, None])\n",
    "        acc += tl.dot(p_ij, v_block)\n",
    "        l_i += tl.sum(p_ij, 1)\n",
    "        \n",
    "        # Update running maximum\n",
    "        m_i = m_new\n",
    "        # --- END OF STUDENT IMPLEMENTATION ---\n",
    "\n",
    "    # --- Phase 2: Diagonal Blocks ---\n",
    "    diag_start = q_block_idx * BLOCK_M\n",
    "    for start_n in range(diag_start, (q_block_idx + 1) * BLOCK_M, BLOCK_N):\n",
    "        # --- STUDENT IMPLEMENTATION: Part 3 - Diagonal blocks with GQA and masking ---\n",
    "        # Load K_j using kv_head_idx\n",
    "        k_offsets = start_n + tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K_ptr + batch_idx * k_stride_b + kv_head_idx * k_stride_h + \\\n",
    "                 (k_offsets[None, :] * k_stride_s + tl.arange(0, HEAD_DIM)[:, None])\n",
    "        k_block = tl.load(k_ptrs, mask=k_offsets[None, :] < SEQ_LEN, other=0.0)\n",
    "        k_block = k_block.to(tl.float32)\n",
    "        \n",
    "        # Compute attention scores S_ij = Q_i * K_j^T\n",
    "        s_ij = tl.dot(q_block, k_block)\n",
    "        s_ij *= qk_scale\n",
    "        \n",
    "        # Apply causal mask: q_idx >= k_idx\n",
    "        causal_mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "        s_ij = tl.where(causal_mask, s_ij, -float('inf'))\n",
    "        \n",
    "        # Load V_j using kv_head_idx\n",
    "        v_ptrs = V_ptr + batch_idx * v_stride_b + kv_head_idx * v_stride_h + \\\n",
    "                 (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        v_block = v_block.to(tl.float32)\n",
    "        \n",
    "        # Online softmax update with masked scores\n",
    "        m_ij = tl.max(s_ij, 1)  # Row-wise max of current scores\n",
    "        m_new = tl.maximum(m_i, m_ij)  # Element-wise max with previous running max\n",
    "        \n",
    "        alpha = tl.exp2(m_i - m_new)  # Rescaling factor for previous values\n",
    "        beta = tl.exp2(m_ij - m_new)  # Rescaling factor for current values\n",
    "        \n",
    "        # Rescale previous accumulator and denominator\n",
    "        acc = acc * alpha[:, None]\n",
    "        l_i = l_i * alpha\n",
    "        \n",
    "        # Compute probabilities and update accumulator\n",
    "        p_ij = tl.exp2(s_ij - m_new[:, None])\n",
    "        acc += tl.dot(p_ij, v_block)\n",
    "        l_i += tl.sum(p_ij, 1)\n",
    "        \n",
    "        # Update running maximum\n",
    "        m_i = m_new\n",
    "        # --- END OF STUDENT IMPLEMENTATION ---\n",
    "\n",
    "    # 4. Normalize and write the final output block.\n",
    "    l_i_safe = l_i[:, None] + 1e-6\n",
    "    acc = acc / l_i_safe\n",
    "    \n",
    "    o_ptrs = O_ptr + batch_idx * q_stride_b + q_head_idx * q_stride_h + \\\n",
    "             (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "             \n",
    "    tl.store(o_ptrs, acc.to(O_ptr.dtype.element_ty), mask=q_offsets[:, None] < SEQ_LEN)\n",
    "\n",
    "\n",
    "def flash_attention_forward(q, k, v, is_causal=True):\n",
    "    \"\"\"\n",
    "    Python wrapper for the GQA-enabled causal FlashAttention kernel.\n",
    "    \"\"\"\n",
    "    batch, n_q_heads, seq_len, head_dim = q.shape\n",
    "    n_kv_heads = k.shape[1]\n",
    "    \n",
    "    assert n_q_heads % n_kv_heads == 0, \"Number of query heads must be divisible by number of K/V heads\"\n",
    "    \n",
    "    o = torch.empty_like(q)\n",
    "    softmax_scale = 1.0 / math.sqrt(head_dim)\n",
    "    \n",
    "    BLOCK_M, BLOCK_N = 128, 64\n",
    "    grid = (triton.cdiv(seq_len, BLOCK_M), batch * n_q_heads)\n",
    "\n",
    "    _flash_attention_forward_gqa_kernel[grid](\n",
    "        q, k, v, o,\n",
    "        q.stride(0), q.stride(1), q.stride(2),\n",
    "        k.stride(0), k.stride(1), k.stride(2),\n",
    "        v.stride(0), v.stride(1), v.stride(2),\n",
    "        softmax_scale,\n",
    "        seq_len,\n",
    "        n_q_heads,\n",
    "        n_kv_heads,\n",
    "        HEAD_DIM=head_dim,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "    )\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder for Problem 5: Grouped-Query Attention\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "def repeat_kv(x, num_groups):\n",
    "    \"\"\"Helper function to repeat K/V heads for GQA naive implementation.\"\"\"\n",
    "    if num_groups == 1:\n",
    "        return x\n",
    "    B, H_kv, N, D = x.shape\n",
    "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
    "    return x.reshape(B, H_kv * num_groups, N, D)\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)   # (seq_len, 1)\n",
    "    col = idx.unsqueeze(0)   # (1, seq_len)\n",
    "\n",
    "    # 1) sliding window:  i - (window_size-1) <= j <= i\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "\n",
    "    # 2) sink at start:   j < sink_size  *and*  j <= i\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
    "    \"\"\"\n",
    "    A correct, robust PyTorch implementation of standard attention for comparison.\n",
    "    Supports GQA, Sliding Window, and Attention Sinks.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_heads_q, seq_len, head_dim = Q.shape\n",
    "    _, num_heads_kv, seq_len, head_dim = K.shape\n",
    "\n",
    "    if num_heads_q != num_heads_kv:\n",
    "        num_groups = num_heads_q // num_heads_kv\n",
    "        K = repeat_kv(K, num_groups)\n",
    "        V = repeat_kv(V, num_groups)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    S = (Q @ K.transpose(-1, -2)) * scale\n",
    "    \n",
    "    if is_causal:\n",
    "        mask = None\n",
    "        if window_size is None: # Causal only\n",
    "            mask = create_mask_bool(seq_len=seq_len, window_size=seq_len, sink_size=0, device=Q.device)\n",
    "        else:\n",
    "            if sink_size is None: # SWA only\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=0, device=Q.device)\n",
    "            else: # SWA + Sink\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=sink_size, device=Q.device)\n",
    "                \n",
    "        S.masked_fill_(~mask, -float('inf'))\n",
    "\n",
    "    P = torch.nn.functional.softmax(S, dim=-1, dtype=torch.float32).to(Q.dtype)\n",
    "    O_final = P @ V\n",
    "    L_final = torch.logsumexp(S.to(torch.float32), dim=-1)\n",
    "    \n",
    "    return O_final, L_final\n",
    "\n",
    "def benchmark_attention(triton_func, naive_func, test_params, is_causal, is_gqa=False, is_swa=False):\n",
    "    \"\"\"Utility to benchmark an attention function and compare it to a naive implementation.\"\"\"\n",
    "    print(\"\\n--- Running Performance Benchmark ---\")\n",
    "    window_size, sink_size = None, None\n",
    "    if is_gqa and not is_swa: # GQA only \n",
    "        batch, heads_q, heads_kv, seq_len, dim = test_params\n",
    "        config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}\"\n",
    "    elif is_swa: # GQA + SWA\n",
    "        batch, heads_q, heads_kv, seq_len, dim, *window_params = test_params\n",
    "        if len(window_params) == 1:\n",
    "            window_size = window_params[0]\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}\"\n",
    "        else:\n",
    "            window_size, sink_size = window_params\n",
    "            config_str = f\"B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}, S={sink_size}\"\n",
    "    else:\n",
    "        batch, heads_q, seq_len, dim = test_params\n",
    "        heads_kv = heads_q\n",
    "        config_str = f\"B={batch}, H={heads_q}, L={seq_len}, D={dim}\"\n",
    "\n",
    "    print(f\"Benchmark Config: {config_str}, Causal={is_causal}\")\n",
    "    \n",
    "    q = torch.randn(batch, heads_q, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    k = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    v = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "\n",
    "    def _run_benchmark(func, is_triton):\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        # Warm-up runs\n",
    "        for _ in range(5):\n",
    "            _ = func(q, k, v, is_causal=is_causal)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        # Timed runs\n",
    "        for _ in range(20):\n",
    "            _ = func(q, k, v, is_causal=is_causal)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time_ms = (end_time - start_time) * 1000 / 20\n",
    "        peak_mem_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        return avg_time_ms, peak_mem_gb\n",
    "\n",
    "    triton_time, triton_mem = _run_benchmark(triton_func, is_triton=True)\n",
    "    # Wrap naive func to discard the L output for benchmarking\n",
    "    naive_wrapper = lambda q, k, v, is_causal: naive_func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)[0]\n",
    "    torch_time, torch_mem = _run_benchmark(naive_wrapper, is_triton=False)\n",
    "\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"{'Implementation':<25} | {'Avg Time (ms)':<20} | {'Peak Memory (GB)':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'PyTorch (Naive)':<25} | {torch_time:<20.4f} | {torch_mem:<20.4f}\")\n",
    "    print(f\"{'Triton (Flash)':<25} | {triton_time:<20.4f} | {triton_mem:<20.4f}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Highlight improvements\n",
    "    speedup = torch_time / triton_time if triton_time > 0 else float('inf')\n",
    "    mem_saving = torch_mem / triton_mem if triton_mem > 0 else float('inf')\n",
    "\n",
    "    print(f\"Triton is {speedup:.2f}x faster than PyTorch (Naive).\")\n",
    "    print(f\"Triton uses {mem_saving:.2f}x less memory.\")\n",
    "\n",
    "def run_correctness_test(test_params, student_func, is_causal, is_gqa=False, is_swa=False, problem_num=None):\n",
    "    \"\"\"Runs a single correctness test case for Triton implementations.\"\"\"\n",
    "    window_size = None\n",
    "    sink_size = None\n",
    "\n",
    "    if is_swa:\n",
    "        batch, heads_q, heads_kv, seq_len, dim, *window_params = test_params\n",
    "        if len(window_params) == 1:\n",
    "            window_size = window_params[0]\n",
    "            param_str = f\"(B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size})\"\n",
    "        elif len(window_params) == 2:\n",
    "            window_size, sink_size = window_params\n",
    "            param_str = f\"(B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim}, W={window_size}, S={sink_size})\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid window_params length: {len(window_params)}\")\n",
    "    elif is_gqa:\n",
    "        batch, heads_q, heads_kv, seq_len, dim = test_params\n",
    "        param_str = f\"(B={batch}, Hq={heads_q}, Hkv={heads_kv}, L={seq_len}, D={dim})\"\n",
    "    else:\n",
    "        batch, heads_q, seq_len, dim = test_params\n",
    "        heads_kv = heads_q\n",
    "        param_str = f\"(B={batch}, H={heads_q}, L={seq_len}, D={dim})\"\n",
    "\n",
    "    q = torch.randn(batch, heads_q, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    k = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    v = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE)\n",
    "    \n",
    "    torch_result, _ = naive_attention(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    if sink_size is not None and window_size is not None:\n",
    "        triton_result = student_func(q, k, v, is_causal=is_causal, window_size=window_size, sink_size=sink_size)\n",
    "    elif window_size is not None:\n",
    "        triton_result = student_func(q, k, v, is_causal=is_causal, window_size=window_size)\n",
    "    else:\n",
    "        triton_result = student_func(q, k, v, is_causal=is_causal)\n",
    "\n",
    "    if torch.allclose(torch_result, triton_result, rtol=5e-2, atol=5e-2):\n",
    "        print(f\"✅ P{problem_num} Correctness Test Passed! {param_str}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ P{problem_num} Correctness Test Failed! {param_str}\")\n",
    "        print(f\" Max diff: {(torch_result - triton_result).abs().max()}\")\n",
    "        return False\n",
    "\n",
    "def check_problem_5():\n",
    "    \"\"\"Checks Problem 5: Grouped-Query Attention.\"\"\"\n",
    "    problem_num = 5\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Grouped-Query Attention ---\")\n",
    "    \n",
    "    torch.manual_seed(46)\n",
    "    # Test cases: (Batch, Heads_Q, Heads_KV, SeqLen, Dim)\n",
    "    test_cases = [\n",
    "        (1, 8, 2, 512, 16),\n",
    "        (1, 8, 2, 1024, 16),\n",
    "        (1, 16, 2, 2048, 16),\n",
    "        (1, 16, 2, 4096, 16),\n",
    "    ]\n",
    "    \n",
    "    results = [run_correctness_test(case, flash_attention_forward, is_causal=True, is_gqa=True, is_swa=False, problem_num=problem_num) for case in test_cases]\n",
    "    if all(results):\n",
    "        print(f\"\\nAll P{problem_num} correctness tests passed!\")\n",
    "        benchmark_attention(flash_attention_forward, naive_attention, test_cases[-1], is_causal=True, is_gqa=True)\n",
    "    \n",
    "    return all(results)\n",
    "\n",
    "# Run the autograder\n",
    "if __name__ == \"__main__\":\n",
    "    check_problem_5()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
