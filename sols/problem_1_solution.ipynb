{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baee89b5",
   "metadata": {},
   "source": [
    "# GStar Assignment 1: FlashAttention2 Implementation\n",
    "\n",
    "This notebook implements FlashAttention-2 forward pass in PyTorch for Problem 1 of the GStar Bootcamp assignment. We'll:\n",
    "\n",
    "1. **Setup Dependencies**: Install required packages for Google Colab\n",
    "2. **Implement FlashAttention2**: Complete the PyTorch implementation with online softmax algorithm\n",
    "3. **Test with Autograder**: Validate correctness against reference implementations\n",
    "\n",
    "## About FlashAttention2\n",
    "\n",
    "FlashAttention-2 is a memory-efficient attention mechanism that uses tiled computation and online softmax to reduce memory usage from O(N²) to O(N) while maintaining mathematical exactness. This implementation uses:\n",
    "\n",
    "- **Tiled computation**: Process attention in blocks to fit in GPU memory\n",
    "- **Online softmax**: Compute softmax incrementally without storing full attention matrix\n",
    "- **Causal masking**: Support for autoregressive models like GPT\n",
    "\n",
    "Let's get started! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b83be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Block 2: Problem 1 - FlashAttention2 PyTorch Implementation\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FlashAttention2Function(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A pure PyTorch implementation of the FlashAttention-2 forward pass.\n",
    "    This version implements the complete online softmax algorithm with tiled computation.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        # Get dimensions from input tensors following the (B, H, N, D) convention\n",
    "        B, H, N_Q, D_H = Q.shape\n",
    "        _, _, N_K, _ = K.shape\n",
    "\n",
    "        # Define tile sizes\n",
    "        Q_TILE_SIZE = 128\n",
    "        K_TILE_SIZE = 128\n",
    "        \n",
    "        N_Q_tiles = math.ceil(N_Q / Q_TILE_SIZE)\n",
    "        N_K_tiles = math.ceil(N_K / K_TILE_SIZE)\n",
    "\n",
    "        # Initialize final output tensors\n",
    "        O_final = torch.zeros_like(Q, dtype=Q.dtype)\n",
    "        L_final = torch.zeros((B, H, N_Q), device=Q.device, dtype=torch.float32)\n",
    "        \n",
    "        scale = 1.0 / math.sqrt(D_H)\n",
    "\n",
    "        # Main loops: Iterate over each batch and head\n",
    "        for b in range(B):\n",
    "            for h in range(H):\n",
    "                Q_bh = Q[b, h, :, :]\n",
    "                K_bh = K[b, h, :, :]\n",
    "                V_bh = V[b, h, :, :]\n",
    "\n",
    "                # Loop over query tiles\n",
    "                for i in range(N_Q_tiles):\n",
    "                    q_start = i * Q_TILE_SIZE\n",
    "                    q_end = min((i + 1) * Q_TILE_SIZE, N_Q)\n",
    "                    Q_tile = Q_bh[q_start:q_end, :]\n",
    "\n",
    "                    # Initialize accumulators for this query tile\n",
    "                    o_i = torch.zeros_like(Q_tile, dtype=torch.float32)  # Use float32 for accumulators\n",
    "                    l_i = torch.zeros(q_end - q_start, device=Q.device, dtype=torch.float32)\n",
    "                    m_i = torch.full((q_end - q_start,), -float('inf'), device=Q.device, dtype=torch.float32)\n",
    "\n",
    "                    # Inner loop over key/value tiles\n",
    "                    for j in range(N_K_tiles):\n",
    "                        k_start = j * K_TILE_SIZE\n",
    "                        k_end = min((j + 1) * K_TILE_SIZE, N_K)\n",
    "\n",
    "                        K_tile = K_bh[k_start:k_end, :]\n",
    "                        V_tile = V_bh[k_start:k_end, :]\n",
    "                        \n",
    "                        S_ij = (Q_tile @ K_tile.transpose(-1, -2)) * scale\n",
    "                        \n",
    "                        # --- STUDENT IMPLEMENTATION STARTS HERE ---\n",
    "                        \n",
    "                        # 1. Apply causal masking if is_causal is True\n",
    "                        if is_causal:\n",
    "                            # Create causal mask for this tile\n",
    "                            q_indices = torch.arange(q_start, q_end, device=Q.device).unsqueeze(1)  # [q_tile_size, 1]\n",
    "                            k_indices = torch.arange(k_start, k_end, device=Q.device).unsqueeze(0)  # [1, k_tile_size]\n",
    "                            causal_mask = q_indices >= k_indices  # True where causal is allowed\n",
    "                            \n",
    "                            # Apply mask: set positions where causal_mask is False to -inf\n",
    "                            S_ij = S_ij.masked_fill(~causal_mask, -float('inf'))\n",
    "                        \n",
    "                        # 2. Compute the new running maximum\n",
    "                        m_ij = torch.max(S_ij, dim=-1)[0]  # Row-wise maximum of current tile\n",
    "                        m_new = torch.maximum(m_i, m_ij)   # Element-wise maximum with previous running max\n",
    "                        \n",
    "                        # 3. Rescale the previous accumulators (o_i, l_i) using the corrected algorithm\n",
    "                        alpha = torch.exp(m_i - m_new)     # Rescaling factor for previous accumulators\n",
    "                        \n",
    "                        # Rescale previous accumulators\n",
    "                        o_i = o_i * alpha.unsqueeze(-1)\n",
    "                        l_i = l_i * alpha\n",
    "                        \n",
    "                        # 4. Compute the probabilities for the current tile, P_tilde_ij = exp(S_ij - m_new)\n",
    "                        P_ij = torch.exp(S_ij - m_new.unsqueeze(-1))\n",
    "                        \n",
    "                        # 5. Accumulate the current tile's contribution to the accumulators\n",
    "                        # Convert V_tile to float32 for precise accumulation\n",
    "                        V_tile_f32 = V_tile.to(torch.float32)\n",
    "                        \n",
    "                        # Update output accumulator: o_i = o_i + P_ij @ V_tile\n",
    "                        o_i = o_i + (P_ij @ V_tile_f32)\n",
    "                        \n",
    "                        # Update normalizer accumulator: l_i = l_i + rowsum(P_ij)\n",
    "                        l_i = l_i + torch.sum(P_ij, dim=-1)\n",
    "                        \n",
    "                        # 6. Update the running max for the next iteration\n",
    "                        m_i = m_new\n",
    "                        \n",
    "                        # --- STUDENT IMPLEMENTATION ENDS HERE ---\n",
    "\n",
    "                    # After iterating through all key tiles, normalize the output\n",
    "                    # This part is provided for you. It handles the final division safely.\n",
    "                    l_i_reciprocal = torch.where(l_i > 0, 1.0 / l_i, 0)\n",
    "                    o_i_normalized = o_i * l_i_reciprocal.unsqueeze(-1)\n",
    "                    \n",
    "                    L_tile = m_i + torch.log(l_i)\n",
    "                    \n",
    "                    # Write results for this tile back to the final output tensors\n",
    "                    O_final[b, h, q_start:q_end, :] = o_i_normalized.to(Q.dtype)\n",
    "                    L_final[b, h, q_start:q_end] = L_tile\n",
    "        \n",
    "        O_final = O_final.to(Q.dtype)\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O_final, L_final)\n",
    "        ctx.is_causal = is_causal\n",
    " \n",
    "        return O_final, L_final\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out, grad_L):\n",
    "        raise NotImplementedError(\"Backward pass not yet implemented for FlashAttention2Function\")\n",
    "\n",
    "print(\"✅ FlashAttention2Function implemented successfully!\")\n",
    "print(\"🎯 Key features implemented:\")\n",
    "print(\"   - Tiled computation with Q_TILE_SIZE=128, K_TILE_SIZE=128\")\n",
    "print(\"   - Online softmax algorithm with running maximum\")\n",
    "print(\"   - Causal masking support for autoregressive models\")\n",
    "print(\"   - Memory-efficient O(N) implementation\")\n",
    "print(\"🔧 Fixed: Corrected online softmax algorithm per FlashAttention-2 paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Block 3: Autograder Testing for Problem 1\n",
    "# ============================================================================\n",
    "\n",
    "# Helper functions and autograder implementation\n",
    "def repeat_kv(x, num_groups):\n",
    "    \"\"\"Helper function to repeat K/V heads for GQA naive implementation.\"\"\"\n",
    "    if num_groups == 1:\n",
    "        return x\n",
    "    B, H_kv, N, D = x.shape\n",
    "    x = x.unsqueeze(2).expand(B, H_kv, num_groups, N, D)\n",
    "    return x.reshape(B, H_kv * num_groups, N, D)\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)   # (seq_len, 1)\n",
    "    col = idx.unsqueeze(0)   # (1, seq_len)\n",
    "\n",
    "    # 1) sliding window:  i - (window_size-1) <= j <= i\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "\n",
    "    # 2) sink at start:   j < sink_size  *and*  j <= i\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(Q, K, V, is_causal=False, window_size=None, sink_size=None):\n",
    "    \"\"\"\n",
    "    A correct, robust PyTorch implementation of standard attention for comparison.\n",
    "    Supports GQA, Sliding Window, and Attention Sinks.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_heads_q, seq_len, head_dim = Q.shape\n",
    "    _, num_heads_kv, seq_len, head_dim = K.shape\n",
    "\n",
    "    if num_heads_q != num_heads_kv:\n",
    "        num_groups = num_heads_q // num_heads_kv\n",
    "        K = repeat_kv(K, num_groups)\n",
    "        V = repeat_kv(V, num_groups)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "    S = (Q @ K.transpose(-1, -2)) * scale\n",
    "    \n",
    "    if is_causal:\n",
    "        mask = None\n",
    "        if window_size is None: # Causal only\n",
    "            mask = create_mask_bool(seq_len=seq_len, window_size=seq_len, sink_size=0, device=Q.device)\n",
    "        else:\n",
    "            if sink_size is None: # SWA only\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=0, device=Q.device)\n",
    "            else: # SWA + Sink\n",
    "                mask = create_mask_bool(seq_len, window_size=window_size, sink_size=sink_size, device=Q.device)\n",
    "                \n",
    "        S.masked_fill_(~mask, -float('inf'))\n",
    "\n",
    "    P = torch.nn.functional.softmax(S, dim=-1, dtype=torch.float32).to(Q.dtype)\n",
    "    O_final = P @ V\n",
    "    L_final = torch.logsumexp(S.to(torch.float32), dim=-1)\n",
    "    \n",
    "    return O_final, L_final\n",
    "\n",
    "def check_problem_1():\n",
    "    \"\"\"Checks Problem 1: PyTorch Tiled Attention.\"\"\"\n",
    "    problem_num = 1\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: Tiled Flash Attention ---\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    test_cases = [\n",
    "        (1, 8, 512, 512, 16, False),\n",
    "        (1, 8, 1024, 1024, 16, True),\n",
    "        (1, 16, 2048, 2048, 16, True),\n",
    "        (1, 16, 4096, 4096, 16, True),\n",
    "    ]\n",
    "    \n",
    "    # Custom test runner for P1 which checks both O and L\n",
    "    def run_p1_test(B, H, N_Q, N_K, D_H, is_causal):\n",
    "        q = torch.randn(B, H, N_Q, D_H, device='cuda', dtype=DTYPE)\n",
    "        k = torch.randn(B, H, N_K, D_H, device='cuda', dtype=DTYPE)\n",
    "        v = torch.randn(B, H, N_K, D_H, device='cuda', dtype=DTYPE)\n",
    "        \n",
    "        naive_O, naive_L = naive_attention(q, k, v, is_causal=is_causal)\n",
    "        student_O, student_L = FlashAttention2Function.apply(q, k, v, is_causal)\n",
    "        \n",
    "        o_match = torch.allclose(naive_O, student_O, rtol=5e-2, atol=5e-2)\n",
    "        l_match = torch.allclose(naive_L, student_L, rtol=5e-2, atol=5e-2)\n",
    "        \n",
    "        param_str = f\"(B={B}, H={H}, Nq={N_Q}, Nk={N_K}, D={D_H}, Causal={is_causal})\"\n",
    "        if o_match and l_match:\n",
    "            print(f\"✅ P{problem_num} Correctness Test Passed! {param_str}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ P{problem_num} Correctness Test Failed! {param_str}\")\n",
    "            if not o_match: print(f\"   Output 'O' mismatch. Max diff: {(naive_O - student_O).abs().max()}\")\n",
    "            if not l_match: print(f\"   Logsumexp 'L' mismatch. Max diff: {(naive_L - student_L).abs().max()}\")\n",
    "            return False\n",
    "\n",
    "    results = [run_p1_test(*case) for case in test_cases]\n",
    "    if all(results):\n",
    "        print(f\"\\n🎉 All P{problem_num} correctness tests passed!\")\n",
    "        print(\"🚀 Your FlashAttention2 implementation is working correctly!\")\n",
    "        \n",
    "        # Run a simple performance benchmark\n",
    "        print(f\"\\n--- Performance Test ---\")\n",
    "        B, H, N, D = 1, 16, 2048, 16\n",
    "        q = torch.randn(B, H, N, D, device='cuda', dtype=DTYPE)\n",
    "        k = torch.randn(B, H, N, D, device='cuda', dtype=DTYPE)\n",
    "        v = torch.randn(B, H, N, D, device='cuda', dtype=DTYPE)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            _ = FlashAttention2Function.apply(q, k, v, True)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):\n",
    "            output, logsumexp = FlashAttention2Function.apply(q, k, v, True)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) * 1000 / 10\n",
    "        print(f\"⚡ Average execution time: {avg_time:.2f} ms\")\n",
    "        print(f\"📊 Test config: B={B}, H={H}, N={N}, D={D}, Causal=True\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n❌ Some tests failed. Please check your implementation.\")\n",
    "    \n",
    "    return all(results)\n",
    "\n",
    "# Run the autograder\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🔥 Starting FlashAttention2 Autograder Tests...\")\n",
    "    success = check_problem_1()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎉 CONGRATULATIONS! 🎉\")\n",
    "        print(\"Your FlashAttention2 implementation passes all tests!\")\n",
    "        print(\"You've successfully implemented:\")\n",
    "        print(\"✅ Tiled computation for memory efficiency\")\n",
    "        print(\"✅ Online softmax algorithm\")\n",
    "        print(\"✅ Causal masking for autoregressive models\")\n",
    "        print(\"✅ Mathematical exactness with reference implementation\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"❌ Some tests failed.\")\n",
    "        print(\"Please review your implementation and try again.\")\n",
    "        print(\"Check the error messages above for specific issues.\")\n",
    "        print(\"=\"*60)\n",
    "else:\n",
    "    print(\"❌ CUDA not available. Cannot run GPU tests.\")\n",
    "    print(\"Please ensure you're running on a CUDA-enabled environment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
