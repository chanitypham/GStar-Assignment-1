{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_9.py - Flash Attention with GQA + Sliding Window + Attention Sinks Backward Pass\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "@triton.jit\n",
    "def _flash_attention_forward_swa_kernel(\n",
    "    # Pointers to Tensors\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, M_ptr,\n",
    "    # Stride information for tensors\n",
    "    q_stride_b, q_stride_h, q_stride_s,\n",
    "    k_stride_b, k_stride_h, k_stride_s,\n",
    "    v_stride_b, v_stride_h, v_stride_s,\n",
    "    o_stride_b, o_stride_h, o_stride_s,\n",
    "    m_stride_b, m_stride_h, m_stride_s,\n",
    "    # Kernel parameters\n",
    "    softmax_scale,\n",
    "    SEQ_LEN,\n",
    "    N_Q_HEADS,\n",
    "    N_KV_HEADS,\n",
    "    WINDOW_SIZE: tl.constexpr,\n",
    "    SINK_SIZE: tl.constexpr,\n",
    "    # Constexpr tile sizes\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    # Get program IDs\n",
    "    q_block_idx = tl.program_id(axis=0)\n",
    "    batch_head_idx = tl.program_id(axis=1)\n",
    "    \n",
    "    batch_idx = batch_head_idx // N_Q_HEADS\n",
    "    q_head_idx = batch_head_idx % N_Q_HEADS\n",
    "\n",
    "    # GQA: Map query head to corresponding K/V head\n",
    "    num_groups = N_Q_HEADS // N_KV_HEADS\n",
    "    kv_head_idx = q_head_idx // num_groups\n",
    "\n",
    "    # Initialize accumulators\n",
    "    m_i = tl.full([BLOCK_M], -float('inf'), dtype=tl.float32)\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # Load query block\n",
    "    q_offsets = q_block_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    q_ptrs = Q_ptr + batch_idx * q_stride_b + q_head_idx * q_stride_h + \\\n",
    "             (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    q_block = tl.load(q_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "\n",
    "    # Determine attention window bounds\n",
    "    query_start = q_block_idx * BLOCK_M\n",
    "    query_end = tl.minimum(query_start + BLOCK_M, SEQ_LEN)\n",
    "    \n",
    "    # Calculate key range based on sliding window and attention sinks\n",
    "    max_query_idx = query_end - 1\n",
    "    \n",
    "    # Keys that can be attended to: \n",
    "    # 1. Sink tokens: [0, SINK_SIZE)\n",
    "    # 2. Sliding window: [max(0, max_query_idx - WINDOW_SIZE + 1), max_query_idx + 1)\n",
    "    window_start = tl.maximum(0, max_query_idx - WINDOW_SIZE + 1)\n",
    "    window_end = max_query_idx + 1\n",
    "    \n",
    "    # Process sink tokens first\n",
    "    if SINK_SIZE > 0:\n",
    "        sink_end_block = tl.cdiv(SINK_SIZE, BLOCK_N)\n",
    "        for k_block_idx in range(sink_end_block):\n",
    "            k_start = k_block_idx * BLOCK_N\n",
    "            k_end = tl.minimum(k_start + BLOCK_N, SINK_SIZE)\n",
    "            \n",
    "            # Load K and V blocks\n",
    "            k_offsets = k_start + tl.arange(0, BLOCK_N)\n",
    "            k_ptrs = K_ptr + batch_idx * k_stride_b + kv_head_idx * k_stride_h + \\\n",
    "                     (k_offsets[:, None] * k_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "            k_block = tl.load(k_ptrs, mask=k_offsets[:, None] < SINK_SIZE, other=0.0)\n",
    "            \n",
    "            v_ptrs = V_ptr + batch_idx * v_stride_b + kv_head_idx * v_stride_h + \\\n",
    "                     (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "            v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SINK_SIZE, other=0.0)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            qk = tl.dot(q_block, tl.trans(k_block))\n",
    "            qk = qk * softmax_scale\n",
    "            \n",
    "            # Apply causal mask\n",
    "            causal_mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "            qk = tl.where(causal_mask, qk, -float('inf'))\n",
    "            \n",
    "            # Update max and normalizer\n",
    "            m_new = tl.maximum(m_i, tl.max(qk, axis=1))\n",
    "            alpha = tl.exp(m_i - m_new)\n",
    "            beta = tl.exp(tl.max(qk, axis=1) - m_new)\n",
    "            l_new = alpha * l_i + beta * tl.sum(tl.exp(qk - m_new[:, None]), axis=1)\n",
    "            \n",
    "            # Update accumulator\n",
    "            acc = acc * alpha[:, None]\n",
    "            p = tl.exp(qk - m_new[:, None])\n",
    "            acc += tl.dot(p, v_block)\n",
    "            \n",
    "            # Update states\n",
    "            m_i = m_new\n",
    "            l_i = l_new\n",
    "\n",
    "    # Process sliding window tokens\n",
    "    window_start_block = tl.maximum(window_start // BLOCK_N, SINK_SIZE // BLOCK_N)\n",
    "    window_end_block = tl.cdiv(window_end, BLOCK_N)\n",
    "    \n",
    "    for k_block_idx in range(window_start_block, window_end_block):\n",
    "        k_start = k_block_idx * BLOCK_N\n",
    "        k_end = tl.minimum(k_start + BLOCK_N, SEQ_LEN)\n",
    "        \n",
    "        # Skip if block is entirely outside attention window\n",
    "        if k_start >= window_end or k_end <= window_start:\n",
    "            continue\n",
    "            \n",
    "        # Skip sink region (already processed)\n",
    "        if k_start < SINK_SIZE:\n",
    "            continue\n",
    "        \n",
    "        # Load K and V blocks\n",
    "        k_offsets = k_start + tl.arange(0, BLOCK_N)\n",
    "        k_ptrs = K_ptr + batch_idx * k_stride_b + kv_head_idx * k_stride_h + \\\n",
    "                 (k_offsets[:, None] * k_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        k_block = tl.load(k_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        \n",
    "        v_ptrs = V_ptr + batch_idx * v_stride_b + kv_head_idx * v_stride_h + \\\n",
    "                 (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "        v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        qk = tl.dot(q_block, tl.trans(k_block))\n",
    "        qk = qk * softmax_scale\n",
    "        \n",
    "        # Apply sliding window + causal mask\n",
    "        sliding_mask = (k_offsets[None, :] >= window_start) & (k_offsets[None, :] < window_end)\n",
    "        causal_mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "        mask = sliding_mask & causal_mask\n",
    "        qk = tl.where(mask, qk, -float('inf'))\n",
    "        \n",
    "        # Update max and normalizer\n",
    "        m_new = tl.maximum(m_i, tl.max(qk, axis=1))\n",
    "        alpha = tl.exp(m_i - m_new)\n",
    "        beta = tl.exp(tl.max(qk, axis=1) - m_new)\n",
    "        l_new = alpha * l_i + beta * tl.sum(tl.exp(qk - m_new[:, None]), axis=1)\n",
    "        \n",
    "        # Update accumulator\n",
    "        acc = acc * alpha[:, None]\n",
    "        p = tl.exp(qk - m_new[:, None])\n",
    "        acc += tl.dot(p, v_block)\n",
    "        \n",
    "        # Update states\n",
    "        m_i = m_new\n",
    "        l_i = l_new\n",
    "\n",
    "    # Normalize output\n",
    "    acc = acc / l_i[:, None]\n",
    "    \n",
    "    # Store output\n",
    "    o_ptrs = O_ptr + batch_idx * o_stride_b + q_head_idx * o_stride_h + \\\n",
    "             (q_offsets[:, None] * o_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    tl.store(o_ptrs, acc.to(O_ptr.dtype.element_ty), mask=q_offsets[:, None] < SEQ_LEN)\n",
    "    \n",
    "    # Store max values\n",
    "    m_ptrs = M_ptr + batch_idx * m_stride_b + q_head_idx * m_stride_h + q_offsets * m_stride_s\n",
    "    tl.store(m_ptrs, m_i, mask=q_offsets < SEQ_LEN)\n",
    "\n",
    "@triton.jit\n",
    "def _flash_attention_backward_swa_kernel(\n",
    "    # In/Out Pointers\n",
    "    Q_ptr, K_ptr, V_ptr, dO_ptr, M_ptr, D_ptr,\n",
    "    dQ_ptr, dK_ptr, dV_ptr,\n",
    "    # Strides\n",
    "    q_stride_b, q_stride_h, q_stride_s,\n",
    "    k_stride_b, k_stride_h, k_stride_s,\n",
    "    v_stride_b, v_stride_h, v_stride_s,\n",
    "    do_stride_b, do_stride_h, do_stride_s,\n",
    "    m_stride_b, m_stride_h, m_stride_s,\n",
    "    d_stride_b, d_stride_h, d_stride_s,\n",
    "    dq_stride_b, dq_stride_h, dq_stride_s,\n",
    "    dk_stride_b, dk_stride_h, dk_stride_s,\n",
    "    dv_stride_b, dv_stride_h, dv_stride_s,\n",
    "    # Parameters\n",
    "    softmax_scale,\n",
    "    BATCH_SIZE: int,\n",
    "    N_Q_HEADS: int,\n",
    "    N_KV_HEADS: int,\n",
    "    SEQ_LEN: int,\n",
    "    WINDOW_SIZE: tl.constexpr,\n",
    "    SINK_SIZE: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    # Tile Sizes\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    # Get current thread block info\n",
    "    kv_block_idx = tl.program_id(axis=0)\n",
    "    batch_head_idx = tl.program_id(axis=1)\n",
    "    \n",
    "    batch_idx = batch_head_idx // N_KV_HEADS\n",
    "    kv_head_idx = batch_head_idx % N_KV_HEADS\n",
    "    \n",
    "    # GQA: Find which query heads use this KV head\n",
    "    num_groups = N_Q_HEADS // N_KV_HEADS\n",
    "    q_head_start = kv_head_idx * num_groups\n",
    "    q_head_end = q_head_start + num_groups\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dk_acc = tl.zeros([BLOCK_N, HEAD_DIM], dtype=tl.float32)\n",
    "    dv_acc = tl.zeros([BLOCK_N, HEAD_DIM], dtype=tl.float32)\n",
    "    \n",
    "    # Load K and V blocks\n",
    "    k_offsets = kv_block_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    k_ptrs = K_ptr + batch_idx * k_stride_b + kv_head_idx * k_stride_h + \\\n",
    "             (k_offsets[:, None] * k_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    k_block = tl.load(k_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "    \n",
    "    v_ptrs = V_ptr + batch_idx * v_stride_b + kv_head_idx * v_stride_h + \\\n",
    "             (k_offsets[:, None] * v_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    v_block = tl.load(v_ptrs, mask=k_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "    \n",
    "    k_start = kv_block_idx * BLOCK_N\n",
    "    k_end = tl.minimum(k_start + BLOCK_N, SEQ_LEN)\n",
    "    \n",
    "    # Determine which query blocks can attend to this key block\n",
    "    # For attention sinks\n",
    "    if k_start < SINK_SIZE:\n",
    "        # Sink tokens can be attended by all queries\n",
    "        q_start_block = 0\n",
    "        q_end_block = tl.cdiv(SEQ_LEN, BLOCK_M)\n",
    "    else:\n",
    "        # Sliding window attention\n",
    "        # Queries that can attend to key k: q in [k, k + WINDOW_SIZE]\n",
    "        max_k_idx = k_end - 1\n",
    "        q_start = tl.maximum(0, max_k_idx)\n",
    "        q_end = tl.minimum(SEQ_LEN, max_k_idx + WINDOW_SIZE)\n",
    "        q_start_block = q_start // BLOCK_M\n",
    "        q_end_block = tl.cdiv(q_end, BLOCK_M)\n",
    "    \n",
    "    # Iterate over query blocks\n",
    "    for q_block_idx in range(q_start_block, q_end_block):\n",
    "        q_start_pos = q_block_idx * BLOCK_M\n",
    "        q_end_pos = tl.minimum(q_start_pos + BLOCK_M, SEQ_LEN)\n",
    "        \n",
    "        # Process each query head in the group\n",
    "        for q_head_idx in range(q_head_start, q_head_end):\n",
    "            # Load Q, dO, M, D for this query head\n",
    "            q_offsets = q_start_pos + tl.arange(0, BLOCK_M)\n",
    "            q_ptrs = Q_ptr + batch_idx * q_stride_b + q_head_idx * q_stride_h + \\\n",
    "                     (q_offsets[:, None] * q_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "            q_block = tl.load(q_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "            \n",
    "            do_ptrs = dO_ptr + batch_idx * do_stride_b + q_head_idx * do_stride_h + \\\n",
    "                      (q_offsets[:, None] * do_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "            do_block = tl.load(do_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "            \n",
    "            m_ptrs = M_ptr + batch_idx * m_stride_b + q_head_idx * m_stride_h + q_offsets * m_stride_s\n",
    "            m_block = tl.load(m_ptrs, mask=q_offsets < SEQ_LEN, other=-float('inf'))\n",
    "            \n",
    "            d_ptrs = D_ptr + batch_idx * d_stride_b + q_head_idx * d_stride_h + q_offsets * d_stride_s\n",
    "            d_block = tl.load(d_ptrs, mask=q_offsets < SEQ_LEN, other=0.0)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            qk = tl.dot(q_block, tl.trans(k_block))\n",
    "            qk = qk * softmax_scale\n",
    "            \n",
    "            # Apply attention mask\n",
    "            if k_start < SINK_SIZE:\n",
    "                # Sink attention: causal mask only\n",
    "                mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "            else:\n",
    "                # Sliding window attention\n",
    "                window_start = tl.maximum(0, tl.max(q_offsets) - WINDOW_SIZE + 1)\n",
    "                window_end = tl.max(q_offsets) + 1\n",
    "                sliding_mask = (k_offsets[None, :] >= window_start) & (k_offsets[None, :] < window_end)\n",
    "                causal_mask = q_offsets[:, None] >= k_offsets[None, :]\n",
    "                mask = sliding_mask & causal_mask\n",
    "            \n",
    "            qk = tl.where(mask, qk, -float('inf'))\n",
    "            \n",
    "            # Compute softmax probabilities\n",
    "            p = tl.exp(qk - m_block[:, None])\n",
    "            \n",
    "            # Compute dp = dO @ V^T\n",
    "            dp = tl.dot(do_block, tl.trans(v_block))\n",
    "            \n",
    "            # Compute ds = p * (dp - D)\n",
    "            ds = p * (dp - d_block[:, None])\n",
    "            ds = ds * softmax_scale\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            dk_acc += tl.dot(tl.trans(ds), q_block)\n",
    "            dv_acc += tl.dot(tl.trans(p), do_block)\n",
    "            \n",
    "            # Compute dQ for this block\n",
    "            dq_block = tl.dot(ds, k_block)\n",
    "            \n",
    "            # Store dQ\n",
    "            dq_ptrs = dQ_ptr + batch_idx * dq_stride_b + q_head_idx * dq_stride_h + \\\n",
    "                      (q_offsets[:, None] * dq_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "            # Load existing dQ and add to it\n",
    "            existing_dq = tl.load(dq_ptrs, mask=q_offsets[:, None] < SEQ_LEN, other=0.0)\n",
    "            tl.store(dq_ptrs, existing_dq + dq_block.to(dQ_ptr.dtype.element_ty), \n",
    "                    mask=q_offsets[:, None] < SEQ_LEN)\n",
    "    \n",
    "    # Store dK and dV\n",
    "    dk_ptrs = dK_ptr + batch_idx * dk_stride_b + kv_head_idx * dk_stride_h + \\\n",
    "              (k_offsets[:, None] * dk_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    tl.store(dk_ptrs, dk_acc.to(dK_ptr.dtype.element_ty), mask=k_offsets[:, None] < SEQ_LEN)\n",
    "    \n",
    "    dv_ptrs = dV_ptr + batch_idx * dv_stride_b + kv_head_idx * dv_stride_h + \\\n",
    "              (k_offsets[:, None] * dv_stride_s + tl.arange(0, HEAD_DIM)[None, :])\n",
    "    tl.store(dv_ptrs, dv_acc.to(dV_ptr.dtype.element_ty), mask=k_offsets[:, None] < SEQ_LEN)\n",
    "\n",
    "class FlashSWDAWithSink(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k, v, window_size, sink_size, is_causal=True, softmax_scale=None):\n",
    "        assert is_causal, \"Currently, only causal attention is supported\"\n",
    "\n",
    "        if softmax_scale is None:\n",
    "            softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "\n",
    "        batch, n_q_heads, seq_len, head_dim = q.shape\n",
    "        _, n_kv_heads, _, _ = k.shape\n",
    "\n",
    "        assert q.shape[0] == v.shape[0] and q.shape[2] == v.shape[2] and q.shape[3] == v.shape[3], \"Query and Value shapes must be compatible except for num_heads\"\n",
    "        assert k.shape[0] == v.shape[0] and k.shape[1] == v.shape[1] and k.shape[2] == v.shape[2] and k.shape[3] == v.shape[3], \"Key and Value shapes must be the same\"\n",
    "        assert head_dim <= 128, \"Head dimension must be less than or equal to 128\"\n",
    "        assert n_q_heads % n_kv_heads == 0, \"Number of query heads must be divisible by number of K/V heads\"\n",
    "\n",
    "        o = torch.empty_like(q)\n",
    "        M = torch.empty((batch, n_q_heads, seq_len), device=q.device, dtype=torch.float32)\n",
    "\n",
    "        BLOCK_M, BLOCK_N = 128, 64\n",
    "        grid = (math.ceil(seq_len / BLOCK_M), batch * n_q_heads)\n",
    "\n",
    "        _flash_attention_forward_swa_kernel[grid](\n",
    "            q, k, v, o, M,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            o.stride(0), o.stride(1), o.stride(2),\n",
    "            M.stride(0), M.stride(1), M.stride(2),\n",
    "            softmax_scale,\n",
    "            seq_len,\n",
    "            n_q_heads,\n",
    "            n_kv_heads,\n",
    "            WINDOW_SIZE=window_size,\n",
    "            SINK_SIZE=sink_size,\n",
    "            HEAD_DIM=head_dim,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.softmax_scale = softmax_scale\n",
    "        ctx.window_size = window_size\n",
    "        ctx.sink_size = sink_size\n",
    "        return o\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, do):\n",
    "        q, k, v, o, M = ctx.saved_tensors\n",
    "        softmax_scale = ctx.softmax_scale\n",
    "        window_size = ctx.window_size\n",
    "        sink_size = ctx.sink_size\n",
    "\n",
    "        batch, n_q_heads, seq_len, head_dim = q.shape\n",
    "        n_kv_heads = k.shape[1]\n",
    "\n",
    "        dq = torch.zeros_like(q)\n",
    "        dk = torch.zeros_like(k)\n",
    "        dv = torch.zeros_like(v)\n",
    "        \n",
    "        # Compute D = rowsum(dO * O)\n",
    "        D = torch.sum(do * o, dim=-1, dtype=torch.float32)\n",
    "        \n",
    "        BLOCK_M, BLOCK_N = 128, 64\n",
    "        grid = (math.ceil(seq_len / BLOCK_N), batch * n_kv_heads)\n",
    "        \n",
    "        _flash_attention_backward_swa_kernel[grid](\n",
    "            q, k, v, do, M, D,\n",
    "            dq, dk, dv,\n",
    "            q.stride(0), q.stride(1), q.stride(2),\n",
    "            k.stride(0), k.stride(1), k.stride(2),\n",
    "            v.stride(0), v.stride(1), v.stride(2),\n",
    "            do.stride(0), do.stride(1), do.stride(2),\n",
    "            M.stride(0), M.stride(1), M.stride(2),\n",
    "            D.stride(0), D.stride(1), D.stride(2),\n",
    "            dq.stride(0), dq.stride(1), dq.stride(2),\n",
    "            dk.stride(0), dk.stride(1), dk.stride(2),\n",
    "            dv.stride(0), dv.stride(1), dv.stride(2),\n",
    "            softmax_scale,\n",
    "            batch,\n",
    "            n_q_heads,\n",
    "            n_kv_heads,\n",
    "            seq_len,\n",
    "            WINDOW_SIZE=window_size,\n",
    "            SINK_SIZE=sink_size,\n",
    "            HEAD_DIM=head_dim,\n",
    "            BLOCK_M=BLOCK_M,\n",
    "            BLOCK_N=BLOCK_N,\n",
    "        )\n",
    "\n",
    "        return dq, dk.to(k.dtype), dv.to(v.dtype), None, None, None, None\n",
    "    \n",
    "def flash_swda_with_sink(q, k, v, window_size: int, sink_size: int = 0, is_causal: bool = True, scale: Optional[float] = None):\n",
    "    return FlashSWDAWithSink.apply(q, k, v, window_size, sink_size, is_causal, scale)\n",
    "\n",
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    B, Hq, Hkv, L, D = 1, 8, 2, 256, 16\n",
    "    window_size, sink_size = 64, 4\n",
    "    \n",
    "    q = torch.randn(B, Hq, L, D, device=device, dtype=DTYPE, requires_grad=True)\n",
    "    k = torch.randn(B, Hkv, L, D, device=device, dtype=DTYPE, requires_grad=True)\n",
    "    v = torch.randn(B, Hkv, L, D, device=device, dtype=DTYPE, requires_grad=True)\n",
    "    \n",
    "    out = flash_swda_with_sink(q, k, v, window_size=window_size, sink_size=sink_size)\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "    print(\"âœ… Problem 9 solution implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226de3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder for Problem 9: FlashAttention-2 with GQA + SWDA + Attention Sinks Backward Pass\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "def create_mask_bool(\n",
    "    seq_len: int,\n",
    "    window_size: int,\n",
    "    sink_size: int,\n",
    "    device=None\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    row = idx.unsqueeze(1)\n",
    "    col = idx.unsqueeze(0)\n",
    "\n",
    "    sliding = (col <= row) & (col >= row - (window_size - 1))\n",
    "    sink = (col < sink_size) & (col <= row)\n",
    "\n",
    "    return sliding | sink\n",
    "\n",
    "def naive_attention(q, k, v, seq_len, window_size, sink_size):\n",
    "    return F.scaled_dot_product_attention(\n",
    "        query=q,\n",
    "        key=k,\n",
    "        value=v,\n",
    "        attn_mask=create_mask_bool(seq_len, window_size, sink_size, device=q.device),\n",
    "        enable_gqa=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "def check_backward_correctness(triton_func, problem_num):\n",
    "    test_cases = [\n",
    "        (1, 16, 16, 4096, 16, 256, 4),\n",
    "        (1, 16, 8, 4096, 16, 256, 4),\n",
    "        (1, 16, 1, 4096, 16, 256, 4),\n",
    "    ]\n",
    "    for case in test_cases:\n",
    "        batch, heads_q, heads_kv, seq_len, dim, window_size, sink_size = case\n",
    "        \n",
    "        if problem_num == 8:\n",
    "            print(f\"Running test case: batch={batch}, heads_q={heads_q}, heads_kv={heads_kv}, seq_len={seq_len}, dim={dim}\")\n",
    "        elif problem_num == 9:\n",
    "            print(f\"Running test case: batch={batch}, heads_q={heads_q}, heads_kv={heads_kv}, seq_len={seq_len}, dim={dim}, window_size={window_size}, sink_size={sink_size}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Problem {problem_num} not supported\")\n",
    "        \n",
    "        q = torch.randn(batch, heads_q, seq_len, dim, device='cuda', dtype=DTYPE, requires_grad=True)\n",
    "        k = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE, requires_grad=True)\n",
    "        v = torch.randn(batch, heads_kv, seq_len, dim, device='cuda', dtype=DTYPE, requires_grad=True)\n",
    "        \n",
    "        q_ref, k_ref, v_ref = q.clone().detach().requires_grad_(), k.clone().detach().requires_grad_(), v.clone().detach().requires_grad_()\n",
    "        \n",
    "        if problem_num == 8:\n",
    "            o_ref = naive_attention(q_ref, k_ref, v_ref, seq_len=seq_len, window_size=seq_len, sink_size=0)\n",
    "            o_triton = triton_func(q, k, v, is_causal=True)\n",
    "        elif problem_num == 9:\n",
    "            o_ref = naive_attention(q_ref, k_ref, v_ref, seq_len, window_size, sink_size)\n",
    "            o_triton = triton_func(q, k, v, window_size=window_size, sink_size=sink_size, is_causal=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Problem {problem_num} not supported\")\n",
    "            \n",
    "        \n",
    "        is_forward_correct = torch.allclose(o_ref, o_triton, atol=1e-2, rtol=1e-2)\n",
    "        if is_forward_correct:\n",
    "            print(\"âœ… Forward Pass Results match\")\n",
    "        else:\n",
    "            print(\"âŒ Forward Pass Results do not match\")\n",
    "        \n",
    "        dout = torch.rand_like(o_ref)\n",
    "        o_ref.backward(dout)\n",
    "        dq_ref, dk_ref, dv_ref = q_ref.grad, k_ref.grad, v_ref.grad\n",
    "        \n",
    "        o_triton.backward(dout)\n",
    "        dq_flash, dk_flash, dv_flash = q.grad, k.grad, v.grad\n",
    "        \n",
    "        is_dq_correct = torch.allclose(dq_ref, dq_flash, atol=5e-2, rtol=5e-2)\n",
    "        is_dk_correct = torch.allclose(dk_ref, dk_flash, atol=5e-2, rtol=5e-2)\n",
    "        is_dv_correct = torch.allclose(dv_ref, dv_flash, atol=5e-2, rtol=5e-2)\n",
    "        if is_dq_correct:\n",
    "            print(\"âœ… Backward Pass Results match on dQ\")\n",
    "        else:\n",
    "            print(\"âŒ Backward Pass Results do not match on dQ\")\n",
    "        if is_dk_correct:\n",
    "            print(\"âœ… Backward Pass Results match on dK\")\n",
    "        else:\n",
    "            print(\"âŒ Backward Pass Results do not match on dK\")\n",
    "        if is_dv_correct:\n",
    "            print(\"âœ… Backward Pass Results match on dV\")\n",
    "        else:\n",
    "            print(\"âŒ Backward Pass Results do not match on dV\")\n",
    "\n",
    "\n",
    "def check_problem_9():\n",
    "    \"\"\"Checks Problem 9: GQA + SWDA + Attention Sinks Backward Pass.\"\"\"\n",
    "    problem_num = 9\n",
    "    print(f\"\\n--- Running Autograder for Problem {problem_num}: GQA + SWDA + Attention Sinks Backward Pass ---\")\n",
    "    \n",
    "    torch.manual_seed(48)\n",
    "    check_backward_correctness(flash_swda_with_sink, problem_num)\n",
    "\n",
    "# Run the autograder\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ðŸš€ Starting Problem 9 Autograder...\")\n",
    "    print(\"ðŸ“ Testing: FlashAttention-2 Triton Implementation with GQA + SWDA + Attention Sinks Backward Pass\")\n",
    "    check_problem_9()\n",
    "else:\n",
    "    print(\"âŒ CUDA not available. Please run this on a GPU-enabled environment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
